# ==================== è¶³çƒæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ v5.2 ====================
# ä¼˜åŒ–ï¼šç›´æ¥ä»HTMLéšè—inputæå–èµ”ç‡æ•°æ®ï¼Œä¿®å¤æ¯”åˆ†æå–ï¼Œä¿æŒåŸå§‹é¡ºåº

import streamlit as st
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re
import time
import json
import os
import pickle
from collections import defaultdict
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ 
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.optimizers import Adam
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False

# æœºå™¨å­¦ä¹ 
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# çˆ¬è™«
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# å¯è§†åŒ–
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ==================== é…ç½® ====================

@dataclass
class Config:
    DATA_FILE: str = "football_training_data.json"
    MODEL_DIR: str = "models_v5"
    MIN_TRAIN_SAMPLES: int = 30
    
    def __post_init__(self):
        os.makedirs(self.MODEL_DIR, exist_ok=True)

CONFIG = Config()

# ==================== æ•°æ®æŒä¹…åŒ–ç®¡ç† ====================

class DataPersistence:
    """æ•°æ®æŒä¹…åŒ–ç®¡ç†å™¨ - è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°"""

    DATA_FILE = "football_data_cache.json"

    def __init__(self):
        self.data = self._load_data()

    def _load_data(self):
        """ä»æœ¬åœ°åŠ è½½æ•°æ®"""
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"ğŸ“‚ ä»æœ¬åœ°åŠ è½½äº† {len(data)} åœºæ¯”èµ›æ•°æ®")

                # è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame(data)

                # è§£æJSONå­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                for col in ['europe', 'asia', 'daxiao', 'handicap']:
                    if col in df.columns:
                        df[col] = df[col].apply(
                            lambda x: json.loads(x) if isinstance(x, str) and x.strip() else 
                                     (x if isinstance(x, list) else [])
                        )

                # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
                required_cols = ['match_id', 'date', 'league', 'time', 'home_team', 'away_team', 'actual_result']
                for col in required_cols:
                    if col not in df.columns:
                        df[col] = ''

                return df
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æœ¬åœ°æ•°æ®å¤±è´¥: {e}")

        # è¿”å›ç©ºDataFrame
        return pd.DataFrame(columns=[
            'match_id', 'date', 'league', 'time', 'status', 'home_team', 'away_team',
            'score', 'score_home', 'score_away', 'actual_result', 'has_result',
            'europe', 'asia', 'daxiao', 'handicap', 'order'
        ])

    def save_data(self):
        """ä¿å­˜æ•°æ®åˆ°æœ¬åœ°"""
        try:
            with open(self.DATA_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False

    def add_matches(self, matches):
        """æ‰¹é‡æ·»åŠ æ¯”èµ›"""
        added, updated = 0, 0
        for match in matches:
            match_id = match.get('match_id')
            existing_idx = None
            for idx, m in enumerate(self.data):
                if m.get('match_id') == match_id:
                    existing_idx = idx
                    break

            if existing_idx is not None:
                existing = self.data[existing_idx]
                for key in ['europe', 'asia', 'handicap', 'daxiao']:
                    if key in match and match[key]:
                        existing[key] = match[key]
                for key, value in match.items():
                    if key not in ['europe', 'asia', 'handicap', 'daxiao']:
                        existing[key] = value
                self.data[existing_idx] = existing
                updated += 1
            else:
                self.data.append(match)
                added += 1

        self.save_data()
        return added, updated

    def get_trainable_matches(self):
        """è·å–å¯ç”¨äºè®­ç»ƒçš„æ¯”èµ›"""
        result = []
        for m in self.data:
            has_result = m.get('actual_result') in ['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ']
            has_odds = any(len(m.get(ot, []) or []) > 0 for ot in ['europe', 'asia', 'handicap', 'daxiao'])
            if has_result and has_odds:
                result.append(m)
        return result

    def get_statistics(self):
        """è·å–ç»Ÿè®¡"""
        trainable = self.get_trainable_matches()
        result_dist = {"ä¸»èƒœ": 0, "å¹³å±€": 0, "å®¢èƒœ": 0}
        for m in trainable:
            r = m.get('actual_result')
            if r in result_dist:
                result_dist[r] += 1
        return {
            'total': len(self.data),
            'trainable': len(trainable),
            'result_distribution': result_dist
        }


# ==================== æ•°æ®é‡‡é›†æ¨¡å— ====================

class DataCollector:
    """ä¿®å¤ç‰ˆæ•°æ®é‡‡é›†å™¨ï¼Œé€‚é…500.comå®é™…HTMLç»“æ„"""

    def __init__(self):
        self.driver = None
        self.base_urls = {
            'live': "https://live.500.com/",
            'europe': "https://odds.500.com/fenxi/ouzhi-{}.shtml",
            'handicap': "https://odds.500.com/fenxi/rangqiu-{}.shtml",
            'asia': "https://odds.500.com/fenxi/yazhi-{}.shtml",
            'daxiao': "https://odds.500.com/fenxi/daxiao-{}.shtml"
        }
        self.log_callback = None

    def set_log_callback(self, callback):
        self.log_callback = callback

    def _log(self, message):
        if self.log_callback:
            self.log_callback(message)
        print(message)

    def get_driver(self):
        if self.driver is not None:
            try:
                self.driver.current_url
                return self.driver
            except:
                self.close()

        try:
            options = Options()
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--disable-gpu")
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
            options.add_experimental_option("prefs", {"profile.managed_default_content_settings.images": 2})

            self.driver = webdriver.Chrome(options=options)
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': 'Object.defineProperty(navigator, "webdriver", {get: () => undefined})'
            })
            self.driver.set_page_load_timeout(30)
            return self.driver
        except Exception as e:
            st.error(f"æµè§ˆå™¨åˆ›å»ºå¤±è´¥: {e}")
            return None

    def close(self):
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None

    def get_page(self, url, wait=3):
        driver = self.get_driver()
        if not driver:
            return None

        try:
            driver.get(url)
            time.sleep(wait)
            for _ in range(2):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(0.5)
            return driver.page_source
        except Exception as e:
            return None

    def fetch_matches_by_date(self, date_str: str, only_finished: bool = True) -> pd.DataFrame:
        """ã€æ ¸å¿ƒä¿®å¤ã€‘è·å–æŒ‡å®šæ—¥æœŸçš„æ¯”èµ›æ•°æ®"""
        html = self.get_page(f"{self.base_urls['live']}?e={date_str}", wait=4)
        if not html:
            return pd.DataFrame()

        soup = BeautifulSoup(html, 'lxml')
        matches = []

        # ã€ä¿®å¤ã€‘ä½¿ç”¨ id="aæ•°å­—" æ ¼å¼åŒ¹é…
        for idx, tr in enumerate(soup.find_all('tr', id=re.compile(r'^a\d+$'))):
            try:
                # ã€ä¿®å¤ã€‘æå–match_idï¼ˆå»æ‰'a'å‰ç¼€ï¼‰
                match_id_full = tr.get('id', '')
                match_id = match_id_full.replace('a', '') if match_id_full else ''
                if not match_id or not match_id.isdigit():
                    continue

                tds = tr.find_all('td')
                if len(tds) < 8:
                    continue

                # ã€ä¿®å¤ã€‘çŠ¶æ€åˆ¤æ–­ï¼ˆç»“åˆstatuså±æ€§å’Œæ–‡æœ¬ï¼‰
                status_code = tr.get('status', '')
                row_text = tr.get_text()

                status = "æœª"
                if status_code == '4' or 'å®Œ' in row_text:
                    status = "å®Œ"
                elif status_code == '2' or 'è¿›è¡Œä¸­' in row_text:
                    status = "è¿›è¡Œä¸­"

                if only_finished and status != "å®Œ":
                    continue

                # æå–è”èµ›
                league = tds[1].get_text(strip=True) if len(tds) > 1 else ""

                # æå–æ—¶é—´
                match_time = ""
                if len(tds) > 3:
                    time_text = tds[3].get_text(strip=True)
                    time_match = re.search(r'(\d{2}:\d{2})', time_text)
                    if time_match:
                        match_time = time_match.group(1)

                # ã€ä¿®å¤ã€‘æå–æ¯”åˆ† - ä»pk divå†…çš„ç¬¬ä¸€ä¸ªå’Œç¬¬ä¸‰ä¸ª<a>æ ‡ç­¾è·å–
                score_home, score_away, actual_result = "", "", ""
                if status == "å®Œ":
                    # æŸ¥æ‰¾åŒ…å«pkç±»çš„div
                    pk_div = tr.find("div", class_="pk")

                    if pk_div:
                        # è·å–æ‰€æœ‰<a>æ ‡ç­¾ï¼Œç¬¬ä¸€ä¸ªæ˜¯ä¸»é˜Ÿæ¯”åˆ†ï¼Œç¬¬ä¸‰ä¸ªæ˜¯å®¢é˜Ÿæ¯”åˆ†
                        all_links = pk_div.find_all("a")
                        if len(all_links) >= 3:
                            try:
                                # ç¬¬ä¸€ä¸ª<a>æ˜¯ä¸»é˜Ÿæ¯”åˆ†
                                home_text = all_links[0].get_text(strip=True)
                                # ç¬¬ä¸‰ä¸ª<a>æ˜¯å®¢é˜Ÿæ¯”åˆ†
                                away_text = all_links[2].get_text(strip=True)

                                home_val = int(home_text)
                                away_val = int(away_text)
                                if 0 <= home_val <= 20 and 0 <= away_val <= 20:
                                    score_home = str(home_val)
                                    score_away = str(away_val)
                            except Exception as e:
                                # é™é»˜å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•
                                pass

                        # å¦‚æœä¸Šé¢çš„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•é€šè¿‡styleé¢œè‰²æŸ¥æ‰¾
                        if not score_home or not score_away:
                            try:
                                # æŸ¥æ‰¾çº¢è‰²æ ·å¼çš„<a>æ ‡ç­¾ï¼ˆé€šå¸¸æ˜¯æ¯”åˆ†ï¼‰
                                red_links = pk_div.find_all("a", style=lambda x: x and "red" in x.lower() if x else False)
                                if len(red_links) >= 2:
                                    home_val = int(red_links[0].get_text(strip=True))
                                    away_val = int(red_links[1].get_text(strip=True))
                                    if 0 <= home_val <= 20 and 0 <= away_val <= 20:
                                        score_home = str(home_val)
                                        score_away = str(away_val)
                            except:
                                pass

                    # ä¼˜å…ˆæ ¹æ®æ¯”åˆ†è®¡ç®—ç»“æœï¼ˆæœ€å¯é ï¼‰
                    if score_home and score_away:
                        try:
                            sh, sa = int(score_home), int(score_away)
                            if sh > sa:
                                actual_result = "ä¸»èƒœ"
                            elif sh < sa:
                                actual_result = "å®¢èƒœ"
                            else:
                                actual_result = "å¹³å±€"
                        except:
                            pass

                    # å¦‚æœæ¯”åˆ†è®¡ç®—å¤±è´¥ï¼Œä»HTMLæå–
                    if not actual_result:
                        red_tds = tr.find_all("td", class_="red")
                        for td in red_tds:
                            result_text = td.get_text(strip=True)
                            # è·³è¿‡æ¯”åˆ†æ ¼å¼ï¼ˆå¦‚"2 - 0"ï¼‰å’Œç©ºæ–‡æœ¬
                            if not result_text or re.match(r'^\d+\s*[-:]\s*\d+$', result_text):
                                continue
                            # åŒ¹é…ç»“æœï¼ˆä»ä¸»é˜Ÿè§†è§’ï¼šèƒœ=ä¸»èƒœï¼Œè´Ÿ=å®¢èƒœï¼Œå¹³=å¹³å±€ï¼‰
                            if result_text in ["ä¸»èƒœ", "å®¢èƒœ", "å¹³å±€"]:
                                actual_result = result_text
                                break
                            elif result_text == "å¹³":
                                actual_result = "å¹³å±€"
                                break
                            elif result_text in ["èƒœ", "ä¸»", "ä¸»èƒœ"]:
                                actual_result = "ä¸»èƒœ"
                                break
                            elif result_text in ["è´Ÿ", "å®¢", "å®¢èƒœ"]:
                                actual_result = "å®¢èƒœ"
                                break
                # ã€ä¿®å¤ã€‘æå–çƒé˜Ÿåç§°
                teams = []

                # æ–¹æ³•1: ä»ç‰¹å®šåˆ—æå–
                for t_idx in [5, 7]:
                    if t_idx < len(tds):
                        links = tds[t_idx].find_all('a')
                        for link in links:
                            name = link.get_text(strip=True)
                            if name and len(name) > 1 and not re.match(r'^\d+(\.\d+)?$', name):
                                if name not in teams:
                                    teams.append(name)
                                    break

                # æ–¹æ³•2: ä»æ‰€æœ‰æ–‡æœ¬æå–
                if len(teams) < 2:
                    for td in tds[2:]:
                        text = td.get_text(strip=True)
                        if (2 <= len(text) <= 15 and 
                            not any(c.isdigit() for c in text) and
                            text not in ['ä¸»', 'å®¢', 'vs', '-', ':', 'åŠçƒ', 'å¹³æ‰‹', 'å—åŠçƒ', 'å—å¹³æ‰‹', 'ä¸€çƒ']):
                            if text not in teams:
                                teams.append(text)
                        if len(teams) >= 2:
                            break

                teams = teams[:2]

                if len(teams) >= 2:
                    matches.append({
                        'order': idx,
                        'match_id': match_id,
                        'date': date_str,
                        'league': league or 'æœªçŸ¥',
                        'time': match_time,
                        'status': status,
                        'home_team': teams[0],
                        'away_team': teams[1],
                        'score': f"{score_home}-{score_away}" if score_home and score_away else "",
                        'score_home': score_home,
                        'score_away': score_away,
                        'actual_result': actual_result,
                        'has_result': status == "å®Œ" and actual_result != ""
                    })

            except Exception as e:
                continue

        df = pd.DataFrame(matches)
        if not df.empty:
            df = df.sort_values('order').reset_index(drop=True)
        return df

    def fetch_odds_from_input(self, match_id: str, odds_type: str = 'europe') -> List[Dict]:
        """ä»HTMLéšè—inputä¸­æå–èµ”ç‡æ•°æ®"""
        url = self.base_urls[odds_type].format(match_id)
        self._log(f"ğŸ“¡ {url}")

        html = self.get_page(url, wait=2)
        if not html:
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        try:
            # æŸ¥æ‰¾éšè—çš„inputå…ƒç´ ï¼Œname="row"
            row_input = soup.find('input', {'name': 'row'})
            if row_input:
                row_value = row_input.get('value', '')
                if row_value:
                    company_rows = row_value.split('|')

                    for row in company_rows:
                        if not row or 'å…¬å¸' in row or 'å¹³å‡' in row:
                            continue

                        parts = row.split(',')
                        if len(parts) >= 7:
                            try:
                                company = {
                                    'company': parts[0].strip(),
                                    'init_home': float(parts[1]),
                                    'init_draw': float(parts[2]),
                                    'init_away': float(parts[3]),
                                    'live_home': float(parts[4]),
                                    'live_draw': float(parts[5]),
                                    'live_away': float(parts[6]),
                                }
                                company['change_home'] = round(company['live_home'] - company['init_home'], 2)
                                company['change_draw'] = round(company['live_draw'] - company['init_draw'], 2)
                                company['change_away'] = round(company['live_away'] - company['init_away'], 2)
                                companies.append(company)
                            except:
                                continue

            # å¤‡ç”¨ï¼šä»è¡¨æ ¼ä¸­æå–
            if not companies:
                table = soup.find('table', id='datatb')
                if table:
                    for tr in table.find_all('tr'):
                        try:
                            tds = tr.find_all('td')
                            if len(tds) < 8:
                                continue

                            name = tds[0].get_text(strip=True)
                            if not name or any(x in name for x in ['å¹³å‡', 'æœ€å¤§', 'å…¬å¸']):
                                continue

                            def parse(txt):
                                try:
                                    return float(re.sub(r'[â†‘â†“]', '', txt))
                                except:
                                    return 0.0

                            company = {
                                'company': name,
                                'init_home': parse(tds[2].get_text()),
                                'init_draw': parse(tds[3].get_text()),
                                'init_away': parse(tds[4].get_text()),
                                'live_home': parse(tds[5].get_text()),
                                'live_draw': parse(tds[6].get_text()),
                                'live_away': parse(tds[7].get_text()),
                            }
                            company['change_home'] = round(company['live_home'] - company['init_home'], 2)
                            company['change_draw'] = round(company['live_draw'] - company['init_draw'], 2)
                            company['change_away'] = round(company['live_away'] - company['init_away'], 2)

                            if company['live_home'] > 0 and company['live_draw'] > 0 and company['live_away'] > 0:
                                companies.append(company)
                        except:
                            continue

        except Exception as e:
            self._log(f"âŒ æå–èµ”ç‡å¤±è´¥: {str(e)[:50]}")

        return companies

    
    
    def fetch_all_odds(self, match_id: str, log_callback=None) -> Dict:
        """
        è·å–æ‰€æœ‰å››ç§èµ”ç‡ - ä¿®å¤è°ƒç”¨é¡ºåº
        """
        if log_callback:
            self.set_log_callback(log_callback)

        odds_data = {
            'europe': [],
            'asia': [],
            'handicap': [],
            'daxiao': []
        }

        self._log(f"ğŸ”„ å¼€å§‹è·å–æ¯”èµ› {match_id} çš„4ç§èµ”ç‡...")

        # 1. æ¬§æ´²èµ”ç‡ï¼ˆç¡®ä¿ç¬¬ä¸€ä¸ªæ‰§è¡Œï¼‰
        self._log(f"ğŸ“Š [1/4] å¼€å§‹è·å–æ¬§æ´²èµ”ç‡...")
        try:
            odds_data['europe'] = self._fetch_europe_odds(match_id)
            self._log(f"âœ“ [1/4] æ¬§æ´²èµ”ç‡: {len(odds_data['europe'])} å®¶å…¬å¸")
        except Exception as e:
            self._log(f"âŒ [1/4] æ¬§æ´²èµ”ç‡è·å–å¤±è´¥: {str(e)[:50]}")
            import traceback
            self._log(f"   é”™è¯¯è¯¦æƒ…: {traceback.format_exc()[:200]}")

        time.sleep(0.5)

        # 2. äºšç›˜
        self._log(f"ğŸ“Š [2/4] å¼€å§‹è·å–äºšç›˜æ•°æ®...")
        try:
            odds_data['asia'] = self._fetch_asia_odds(match_id)
            self._log(f"âœ“ [2/4] äºšç›˜: {len(odds_data['asia'])} å®¶å…¬å¸")
        except Exception as e:
            self._log(f"âŒ [2/4] äºšç›˜è·å–å¤±è´¥: {str(e)[:50]}")

        time.sleep(0.5)

        # 3. è®©çƒèƒœå¹³è´Ÿ
        self._log(f"ğŸ“Š [3/4] å¼€å§‹è·å–è®©çƒèƒœå¹³è´Ÿ...")
        try:
            odds_data['handicap'] = self._fetch_handicap_odds(match_id)
            self._log(f"âœ“ [3/4] è®©çƒ: {len(odds_data['handicap'])} å®¶å…¬å¸")
        except Exception as e:
            self._log(f"âŒ [3/4] è®©çƒè·å–å¤±è´¥: {str(e)[:50]}")

        time.sleep(0.5)

        # 4. å¤§å°çƒ
        self._log(f"ğŸ“Š [4/4] å¼€å§‹è·å–å¤§å°çƒæ•°æ®...")
        try:
            odds_data['daxiao'] = self._fetch_daxiao_odds(match_id)
            self._log(f"âœ“ [4/4] å¤§å°çƒ: {len(odds_data['daxiao'])} å®¶å…¬å¸")
        except Exception as e:
            self._log(f"âŒ [4/4] å¤§å°çƒè·å–å¤±è´¥: {str(e)[:50]}")

        total = sum(len(v) for v in odds_data.values())
        self._log(f"âœ… æ€»è®¡: {total} æ¡èµ”ç‡æ•°æ® (æ¬§èµ”:{len(odds_data['europe'])}, äºšç›˜:{len(odds_data['asia'])}, è®©çƒ:{len(odds_data['handicap'])}, å¤§å°çƒ:{len(odds_data['daxiao'])})")

        return odds_data

    
    
    
    
    def _fetch_europe_odds(self, match_id: str) -> List[Dict]:
        """
        è·å–æ¬§æ´²èµ”ç‡ - æ ¹æ®å®é™…HTMLç»“æ„ç²¾ç¡®æå–
        """
        url = f"https://odds.500.com/fenxi/ouzhi-{match_id}.shtml"
        self._log(f"      ğŸŒ è®¿é—®: {url}")

        html = self.get_page(url, wait=4)
        if not html:
            self._log(f"      âŒ é¡µé¢ä¸ºç©º")
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        # æŸ¥æ‰¾æ‰€æœ‰æ•°æ®è¡Œï¼ˆæ ¹æ®å®é™…HTMLï¼štr[class="tr1"] æˆ– tr[class="tr2"]ï¼‰
        rows = soup.find_all('tr', {'class': ['tr1', 'tr2']})
        self._log(f"      ğŸ“‹ æ‰¾åˆ° {len(rows)} ä¸ªæ•°æ®è¡Œ")

        for tr in rows:
            try:
                # æå–å…¬å¸å
                company_name = ""
                name_td = tr.find('td', {'class': 'tb_plgs'})
                if name_td:
                    span = name_td.find('span', {'class': 'quancheng'})
                    if span:
                        company_name = span.get_text(strip=True)

                if not company_name or any(x in company_name for x in ['å¹³å‡', 'æœ€å¤§', 'æœ€å°']):
                    continue

                # æŸ¥æ‰¾åŒ…å«èµ”ç‡çš„åµŒå¥—è¡¨æ ¼
                # æ ¹æ®HTMLç»“æ„ï¼Œèµ”ç‡åœ¨ç¬¬3ä¸ª<td>ä¸­ï¼ˆç´¢å¼•2ï¼‰
                tds = tr.find_all('td', recursive=False)
                if len(tds) < 3:
                    continue

                odds_td = tds[2]  # ç¬¬3ä¸ª<td>åŒ…å«èµ”ç‡è¡¨æ ¼

                # æŸ¥æ‰¾åµŒå¥—çš„èµ”ç‡è¡¨æ ¼
                inner_table = odds_td.find('table', {'class': 'pl_table_data'})
                if not inner_table:
                    continue

                # æŸ¥æ‰¾æ‰€æœ‰è¡Œ
                inner_rows = inner_table.find_all('tr')
                if len(inner_rows) < 2:
                    continue

                # ç¬¬ä¸€è¡Œï¼šåˆå§‹èµ”ç‡
                init_row = inner_rows[0]
                init_tds = init_row.find_all('td')
                if len(init_tds) < 3:
                    continue

                init_home = self._parse_odds_value(init_tds[0])
                init_draw = self._parse_odds_value(init_tds[1])
                init_away = self._parse_odds_value(init_tds[2])

                # ç¬¬äºŒè¡Œï¼šå³æ—¶èµ”ç‡
                live_row = inner_rows[1]
                live_tds = live_row.find_all('td')
                if len(live_tds) < 3:
                    continue

                live_home = self._parse_odds_value(live_tds[0])
                live_draw = self._parse_odds_value(live_tds[1])
                live_away = self._parse_odds_value(live_tds[2])

                # éªŒè¯æ•°æ®
                if all(v > 0 for v in [init_home, init_draw, init_away, live_home, live_draw, live_away]):
                    companies.append({
                        'company': company_name,
                        'init_home': round(init_home, 2),
                        'init_draw': round(init_draw, 2),
                        'init_away': round(init_away, 2),
                        'live_home': round(live_home, 2),
                        'live_draw': round(live_draw, 2),
                        'live_away': round(live_away, 2),
                        'change_home': round(live_home - init_home, 2),
                        'change_draw': round(live_draw - init_draw, 2),
                        'change_away': round(live_away - init_away, 2)
                    })

                    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
                    if len(companies) == 1:
                        self._log(f"      âœ“ æ ·æœ¬: {company_name} | åˆ:{init_home}/{init_draw}/{init_away} | å³:{live_home}/{live_draw}/{live_away}")

            except Exception as e:
                continue

        self._log(f"      âœ… å…±è·å– {len(companies)} å®¶å…¬å¸")
        return companies

    def _parse_odds_value(self, td) -> float:
        """ä»tdå…ƒç´ è§£æèµ”ç‡å€¼"""
        try:
            # è·å–æ–‡æœ¬
            text = td.get_text(strip=True)
            # æ¸…ç†
            text = re.sub(r'[â†‘â†“]', '', text).strip()
            # è½¬æ¢
            return float(text)
        except:
            return 0.0

    def _parse_odds_value(self, td) -> float:
        """ä»tdå…ƒç´ è§£æèµ”ç‡å€¼"""
        try:
            text = td.get_text(strip=True)
            text = re.sub(r'[â†‘â†“]', '', text).strip()
            return float(text)
        except:
            return 0.0

    def _fetch_asia_odds(self, match_id: str) -> List[Dict]:
        """
        è·å–äºšç›˜æ•°æ®
        æ•°æ®æ ¼å¼: å…¬å¸å, åˆå§‹ä¸»æ°´, åˆå§‹ç›˜å£, åˆå§‹å®¢æ°´, å³æ—¶ä¸»æ°´, å³æ—¶ç›˜å£, å³æ—¶å®¢æ°´
        """
        url = f"https://odds.500.com/fenxi/yazhi-{match_id}.shtml"
        self._log(f"      ğŸŒ è®¿é—®: {url}")

        html = self.get_page(url, wait=3)
        if not html:
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        try:
            # ä»éšè—inputè·å–
            row_input = soup.find('input', {'name': 'row'})
            if row_input:
                row_value = row_input.get('value', '')
                if row_value:
                    for row in row_value.split('|'):
                        if not row or 'å…¬å¸' in row:
                            continue
                        parts = row.split(',')
                        if len(parts) >= 7:
                            try:
                                companies.append({
                                    'company': parts[0].strip(),
                                    'init_home': float(parts[1]),      # åˆå§‹ä¸»æ°´
                                    'init_handicap': parts[2].strip(),  # åˆå§‹ç›˜å£
                                    'init_away': float(parts[3]),       # åˆå§‹å®¢æ°´
                                    'live_home': float(parts[4]),       # å³æ—¶ä¸»æ°´
                                    'live_handicap': parts[5].strip(),  # å³æ—¶ç›˜å£
                                    'live_away': float(parts[6])        # å³æ—¶å®¢æ°´
                                })
                            except:
                                continue

            # å¤‡ç”¨ï¼šä»è¡¨æ ¼è·å–
            if not companies:
                table = soup.find('table', id='datatb')
                if table:
                    for tr in table.find_all('tr')[1:]:
                        tds = tr.find_all('td')
                        if len(tds) >= 8:
                            try:
                                name = tds[0].get_text(strip=True)
                                if not name or any(x in name for x in ['å¹³å‡', 'æœ€å¤§']):
                                    continue

                                companies.append({
                                    'company': name,
                                    'init_home': self._parse_float(tds[2].get_text()),
                                    'init_handicap': tds[3].get_text(strip=True),
                                    'init_away': self._parse_float(tds[4].get_text()),
                                    'live_home': self._parse_float(tds[5].get_text()),
                                    'live_handicap': tds[6].get_text(strip=True),
                                    'live_away': self._parse_float(tds[7].get_text())
                                })
                            except:
                                continue
        except Exception as e:
            self._log(f"      âŒ æå–å¤±è´¥: {str(e)[:50]}")

        return companies

    def _fetch_handicap_odds(self, match_id: str) -> List[Dict]:
        """
        è·å–è®©çƒèƒœå¹³è´Ÿï¼ˆç«å½©ï¼‰
        æ•°æ®æ ¼å¼: å…¬å¸å, è®©çƒæ•°, åˆå§‹ä¸»èƒœ, åˆå§‹å¹³å±€, åˆå§‹å®¢èƒœ, å³æ—¶ä¸»èƒœ, å³æ—¶å¹³å±€, å³æ—¶å®¢èƒœ
        """
        url = f"https://odds.500.com/fenxi/rangqiu-{match_id}.shtml"
        self._log(f"      ğŸŒ è®¿é—®: {url}")

        html = self.get_page(url, wait=3)
        if not html:
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        try:
            row_input = soup.find('input', {'name': 'row'})
            if row_input:
                row_value = row_input.get('value', '')
                if row_value:
                    for row in row_value.split('|'):
                        if not row or 'å…¬å¸' in row:
                            continue
                        parts = row.split(',')
                        if len(parts) >= 8:
                            try:
                                companies.append({
                                    'company': parts[0].strip(),
                                    'handicap': parts[1].strip(),       # è®©çƒæ•°
                                    'init_home': float(parts[2]),       # åˆå§‹è®©çƒä¸»èƒœ
                                    'init_draw': float(parts[3]),       # åˆå§‹è®©çƒå¹³å±€
                                    'init_away': float(parts[4]),       # åˆå§‹è®©çƒå®¢èƒœ
                                    'live_home': float(parts[5]),       # å³æ—¶è®©çƒä¸»èƒœ
                                    'live_draw': float(parts[6]),       # å³æ—¶è®©çƒå¹³å±€
                                    'live_away': float(parts[7])        # å³æ—¶è®©çƒå®¢èƒœ
                                })
                            except:
                                continue

            # å¤‡ç”¨
            if not companies:
                table = soup.find('table', id='datatb')
                if table:
                    for tr in table.find_all('tr')[1:]:
                        tds = tr.find_all('td')
                        if len(tds) >= 9:
                            try:
                                name = tds[0].get_text(strip=True)
                                if not name or any(x in name for x in ['å¹³å‡', 'æœ€å¤§']):
                                    continue

                                companies.append({
                                    'company': name,
                                    'handicap': tds[1].get_text(strip=True),
                                    'init_home': self._parse_float(tds[3].get_text()),
                                    'init_draw': self._parse_float(tds[4].get_text()),
                                    'init_away': self._parse_float(tds[5].get_text()),
                                    'live_home': self._parse_float(tds[6].get_text()),
                                    'live_draw': self._parse_float(tds[7].get_text()),
                                    'live_away': self._parse_float(tds[8].get_text())
                                })
                            except:
                                continue
        except Exception as e:
            self._log(f"      âŒ æå–å¤±è´¥: {str(e)[:50]}")

        return companies

    def _fetch_daxiao_odds(self, match_id: str) -> List[Dict]:
        """
        è·å–å¤§å°çƒæ•°æ®
        æ•°æ®æ ¼å¼: å…¬å¸å, åˆå§‹å¤§çƒæ°´, åˆå§‹ç›˜å£, åˆå§‹å°çƒæ°´, å³æ—¶å¤§çƒæ°´, å³æ—¶ç›˜å£, å³æ—¶å°çƒæ°´
        """
        url = f"https://odds.500.com/fenxi/daxiao-{match_id}.shtml"
        self._log(f"      ğŸŒ è®¿é—®: {url}")

        html = self.get_page(url, wait=3)
        if not html:
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        try:
            row_input = soup.find('input', {'name': 'row'})
            if row_input:
                row_value = row_input.get('value', '')
                if row_value:
                    for row in row_value.split('|'):
                        if not row or 'å…¬å¸' in row:
                            continue
                        parts = row.split(',')
                        if len(parts) >= 7:
                            try:
                                companies.append({
                                    'company': parts[0].strip(),
                                    'init_over': float(parts[1]),       # åˆå§‹å¤§çƒæ°´
                                    'init_line': parts[2].strip(),      # åˆå§‹ç›˜å£
                                    'init_under': float(parts[3]),      # åˆå§‹å°çƒæ°´
                                    'live_over': float(parts[4]),       # å³æ—¶å¤§çƒæ°´
                                    'live_line': parts[5].strip(),      # å³æ—¶ç›˜å£
                                    'live_under': float(parts[6])       # å³æ—¶å°çƒæ°´
                                })
                            except:
                                continue

            # å¤‡ç”¨
            if not companies:
                table = soup.find('table', id='datatb')
                if table:
                    for tr in table.find_all('tr')[1:]:
                        tds = tr.find_all('td')
                        if len(tds) >= 8:
                            try:
                                name = tds[0].get_text(strip=True)
                                if not name or any(x in name for x in ['å¹³å‡', 'æœ€å¤§']):
                                    continue

                                companies.append({
                                    'company': name,
                                    'init_over': self._parse_float(tds[2].get_text()),
                                    'init_line': tds[3].get_text(strip=True),
                                    'init_under': self._parse_float(tds[4].get_text()),
                                    'live_over': self._parse_float(tds[5].get_text()),
                                    'live_line': tds[6].get_text(strip=True),
                                    'live_under': self._parse_float(tds[7].get_text())
                                })
                            except:
                                continue
        except Exception as e:
            self._log(f"      âŒ æå–å¤±è´¥: {str(e)[:50]}")

        return companies

    def _parse_float(self, text: str) -> float:
        """è§£ææµ®ç‚¹æ•°ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦"""
        text = re.sub(r'[â†‘â†“]', '', text).strip()
        try:
            return float(text)
        except:
            return 0.0

    def batch_fetch_history(self, start_date: str, days: int = 7, progress_callback=None, log_callback=None) -> pd.DataFrame:
        """æ‰¹é‡è·å–å†å²æ•°æ®"""
        if log_callback:
            self.set_log_callback(log_callback)

        all_matches = []
        start = datetime.strptime(start_date, '%Y-%m-%d')

        for i in range(days):
            current_date = start - timedelta(days=i)
            date_str = current_date.strftime('%Y-%m-%d')

            self._log(f"ğŸ“… è·å– {date_str} çš„æ¯”èµ›...")

            if progress_callback:
                progress_callback(i+1, days, date_str, "è·å–æ¯”èµ›åˆ—è¡¨...")

            matches = self.fetch_matches_by_date(date_str, only_finished=True)

            if not matches.empty:
                self._log(f"ğŸ“ æ‰¾åˆ° {len(matches)} åœºå®Œèµ›æ¯”èµ›")

                if progress_callback:
                    progress_callback(i+1, days, date_str, f"è·å– {len(matches)} åœºæ¯”èµ›èµ”ç‡...")

                match_list = []
                for idx, row in matches.iterrows():
                    if progress_callback:
                        progress_callback(i+1, days, date_str, f"[{idx+1}/{len(matches)}] {row['home_team']} vs {row['away_team']}")

                    odds = self.fetch_all_odds(row['match_id'], log_callback)

                    match_data = row.to_dict()
                    match_data.update(odds)
                    match_list.append(match_data)

                    time.sleep(0.2)

                all_matches.extend(match_list)
                self._log(f"âœ… {date_str} å®Œæˆï¼Œå…± {len(match_list)} åœº")
            else:
                self._log(f"âš ï¸ {date_str} æ— å®Œèµ›æ•°æ®")

            time.sleep(0.5)

        df = pd.DataFrame(all_matches)
        if not df.empty:
            df = df.sort_values(['date', 'order']).reset_index(drop=True)
        return df


    def fetch_future_matches(self, date_str: str) -> pd.DataFrame:
        """è·å–æŒ‡å®šæ—¥æœŸçš„æœªæ¥æ¯”èµ›ï¼ˆæœªå¼€å§‹æˆ–è¿›è¡Œä¸­ï¼‰"""
        html = self.get_page(f"{self.base_urls['live']}?e={date_str}", wait=4)
        if not html:
            return pd.DataFrame()

        soup = BeautifulSoup(html, 'lxml')
        matches = []

        for idx, tr in enumerate(soup.find_all('tr', id=re.compile(r'^a\d+$'))):
            try:
                match_id_full = tr.get('id', '')
                match_id = match_id_full.replace('a', '') if match_id_full else ''
                if not match_id or not match_id.isdigit():
                    continue

                tds = tr.find_all('td')
                if len(tds) < 8:
                    continue

                status_code = tr.get('status', '')
                row_text = tr.get_text()

                status = "æœª"
                if status_code == '4' or 'å®Œ' in row_text:
                    status = "å®Œ"
                elif status_code == '2' or 'è¿›è¡Œä¸­' in row_text:
                    status = "è¿›è¡Œä¸­"

                # åªè·å–æœªå¼€å§‹æˆ–è¿›è¡Œä¸­çš„æ¯”èµ›
                if status == "å®Œ":
                    continue

                league = tds[1].get_text(strip=True) if len(tds) > 1 else ""

                match_time = ""
                if len(tds) > 3:
                    time_text = tds[3].get_text(strip=True)
                    time_match = re.search(r'(\d{2}:\d{2})', time_text)
                    if time_match:
                        match_time = time_match.group(1)

                # æå–çƒé˜Ÿåç§°
                teams = []
                for t_idx in [5, 7]:
                    if t_idx < len(tds):
                        links = tds[t_idx].find_all('a')
                        for link in links:
                            name = link.get_text(strip=True)
                            if name and len(name) > 1 and not re.match(r'^\d+(\.\d+)?$', name):
                                if name not in teams:
                                    teams.append(name)
                                    break

                if len(teams) < 2:
                    for td in tds[2:]:
                        text = td.get_text(strip=True)
                        if (2 <= len(text) <= 15 and 
                            not any(c.isdigit() for c in text) and
                            text not in ['ä¸»', 'å®¢', 'vs', '-', ':']):
                            if text not in teams:
                                teams.append(text)
                        if len(teams) >= 2:
                            break

                teams = teams[:2]

                if len(teams) >= 2:
                    matches.append({
                        'order': idx,
                        'match_id': match_id,
                        'date': date_str,
                        'league': league or 'æœªçŸ¥',
                        'time': match_time,
                        'status': status,
                        'home_team': teams[0],
                        'away_team': teams[1],
                        'score': "",
                        'score_home': "",
                        'score_away': "",
                        'actual_result': "",  # æœªæ¥æ¯”èµ›æ²¡æœ‰ç»“æœ
                        'has_result': False
                    })

            except Exception as e:
                continue

        df = pd.DataFrame(matches)
        if not df.empty:
            df = df.sort_values('order').reset_index(drop=True)
        return df

    def fetch_future_matches_with_odds(self, date_str: str, progress_callback=None, log_callback=None) -> pd.DataFrame:
        """è·å–æœªæ¥æ¯”èµ›å¹¶è·å–èµ”ç‡æ•°æ®"""
        if log_callback:
            self.set_log_callback(log_callback)

        self._log(f"ğŸ”® è·å– {date_str} çš„æœªæ¥æ¯”èµ›...")

        matches = self.fetch_future_matches(date_str)
        if matches.empty:
            self._log(f"âš ï¸ {date_str} æ²¡æœ‰æœªæ¥æ¯”èµ›")
            return pd.DataFrame()

        self._log(f"ğŸ“ æ‰¾åˆ° {len(matches)} åœºæœªæ¥æ¯”èµ›")

        match_list = []
        for idx, row in matches.iterrows():
            if progress_callback:
                progress_callback(idx+1, len(matches), date_str, f"{row['home_team']} vs {row['away_team']}")

            self._log(f"ğŸ“Š è·å–èµ”ç‡: {row['home_team']} vs {row['away_team']}")

            odds = self.fetch_all_odds(row['match_id'], log_callback)

            match_data = row.to_dict()
            match_data.update(odds)
            match_list.append(match_data)

            time.sleep(0.2)

        df = pd.DataFrame(match_list)
        if not df.empty:
            df = df.sort_values('order').reset_index(drop=True)

        self._log(f"âœ… {date_str} å®Œæˆï¼Œå…± {len(df)} åœº")
        return df
class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹ - å¢å¼ºç‰ˆï¼Œå¤„ç†ç»´åº¦é—®é¢˜å’Œç±»åˆ«ä¸å¹³è¡¡"""

    FEATURE_DIM = 85

    def __init__(self):
        self.scaler = StandardScaler()
        self.is_fitted = False

    def extract_features(self, match_data: Dict) -> np.ndarray:
        """æå–ç‰¹å¾ - ç¡®ä¿è¾“å‡ºå›ºå®š85ç»´"""
        features = []

        # 1. æ¬§æ´²èµ”ç‡ç‰¹å¾ (35ç»´)
        europe = match_data.get('europe', [])
        features.extend(self._europe_features(europe))

        # 2. äºšç›˜ç‰¹å¾ (15ç»´)
        asia = match_data.get('asia', [])
        features.extend(self._asia_features(asia))

        # 3. è®©çƒç‰¹å¾ (10ç»´)
        handicap = match_data.get('handicap', [])
        features.extend(self._handicap_features(handicap))

        # 4. å¤§å°çƒç‰¹å¾ (12ç»´)
        daxiao = match_data.get('daxiao', [])
        features.extend(self._daxiao_features(daxiao))

        # 5. å…ƒç‰¹å¾ (13ç»´)
        features.extend(self._meta_features(match_data))

        # ã€å…³é”®ä¿®å¤ã€‘ç¡®ä¿ç»´åº¦æ­£å¥½æ˜¯85
        if len(features) > self.FEATURE_DIM:
            features = features[:self.FEATURE_DIM]
        elif len(features) < self.FEATURE_DIM:
            features.extend([0.0] * (self.FEATURE_DIM - len(features)))

        return np.array(features)

    def _europe_features(self, europe_odds):
        """æ¬§æ´²èµ”ç‡ç‰¹å¾ - å›ºå®š35ç»´"""
        if not europe_odds or len(europe_odds) == 0:
            return [0.0] * 35

        try:
            df = pd.DataFrame(europe_odds)
            features = []

            # åŸºç¡€ç»Ÿè®¡ (15ç»´)
            for col in ['live_home', 'live_draw', 'live_away']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()), 
                        float(df[col].std()) if len(df) > 1 else 0.0, 
                        float(df[col].min()), 
                        float(df[col].max()), 
                        float(df[col].median())
                    ])
                else:
                    features.extend([0.0] * 5)

            # å˜åŒ–è¶‹åŠ¿ (6ç»´)
            for col in ['change_home', 'change_draw', 'change_away']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()),
                        (df[col] < 0).sum() / len(df) if len(df) > 0 else 0.0
                    ])
                else:
                    features.extend([0.0, 0.0])

            # å‡¯åˆ©æŒ‡æ•°ç›¸å…³ (14ç»´)
            if 'live_home' in df.columns and 'live_draw' in df.columns and 'live_away' in df.columns:
                total_prob = 1/df['live_home'] + 1/df['live_draw'] + 1/df['live_away']
                features.extend([
                    float(total_prob.mean()),
                    float(total_prob.std()) if len(total_prob) > 1 else 0.0,
                    float((1/df['live_home']).mean()),
                    float((1/df['live_draw']).mean()),
                    float((1/df['live_away']).mean()),
                    float((1/df['live_home']).std()) if len(df) > 1 else 0.0,
                    float((1/df['live_draw']).std()) if len(df) > 1 else 0.0,
                    float((1/df['live_away']).std()) if len(df) > 1 else 0.0,
                    (total_prob > 1.1).sum() / len(df),
                    (total_prob < 0.95).sum() / len(df),
                    float(df['live_home'].mean() / df['live_away'].mean()) if df['live_away'].mean() != 0 else 1.0,
                    float((df['live_home'] < df['live_away']).sum() / len(df)),
                    float(df['live_draw'].mean()),
                    float(len(df))
                ])
            else:
                features.extend([0.0] * 14)

            return features[:35]
        except Exception as e:
            return [0.0] * 35 * 35

    def _asia_features(self, asia_odds):
        """äºšç›˜ç‰¹å¾ - å›ºå®š15ç»´"""
        if not asia_odds or len(asia_odds) == 0:
            return [0.0] * 15

        try:
            df = pd.DataFrame(asia_odds)
            features = []

            # æ°´ä½ç»Ÿè®¡ (8ç»´)
            for col in ['live_home', 'live_away']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()), 
                        float(df[col].std()) if len(df) > 1 else 0.0, 
                        float(df[col].min()), 
                        float(df[col].max())
                    ])
                else:
                    features.extend([0.0] * 4)

            # ç›˜å£åˆ†æ (7ç»´)
            if 'live_handicap' in df.columns:
                handicaps = []
                for h in df['live_handicap']:
                    try:
                        h_str = str(h).replace('çƒ', '').replace('åŠ', '.5').replace('å¹³', '0')
                        handicaps.append(float(h_str))
                    except:
                        handicaps.append(0)

                if len(handicaps) > 0:
                    features.extend([
                        float(np.mean(handicaps)),
                        float(np.std(handicaps)) if len(handicaps) > 1 else 0.0,
                        float(max(handicaps)),
                        float(min(handicaps)),
                        (np.array(handicaps) > 0).sum() / len(handicaps),
                        (np.array(handicaps) < 0).sum() / len(handicaps),
                        len(set(handicaps)) / len(handicaps) if len(handicaps) > 0 else 0
                    ])
                else:
                    features.extend([0.0] * 7)
            else:
                features.extend([0.0] * 7)

            return features[:15]
        except Exception as e:
            return [0.0] * 15 * 15

    def _handicap_features(self, handicap_odds):
        """è®©çƒç‰¹å¾ - å›ºå®š10ç»´"""
        if not handicap_odds or len(handicap_odds) == 0:
            return [0.0] * 10

        try:
            df = pd.DataFrame(handicap_odds)
            features = []

            for col in ['live_home', 'live_draw', 'live_away']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()), 
                        float(df[col].std()) if len(df) > 1 else 0.0, 
                        float(df[col].min()), 
                        float(df[col].max())
                    ])
                else:
                    features.extend([0.0] * 4)

            if 'live_home' in df.columns and 'live_away' in df.columns and len(df) > 0:
                features.extend([
                    (df['live_home'] < df['live_away']).sum() / len(df),
                    (df['live_home'] > df['live_away']).sum() / len(df)
                ])
            else:
                features.extend([0.0, 0.0])

            return features[:10]
        except Exception as e:
            return [0.0] * 10 * 10

    def _daxiao_features(self, daxiao_odds):
        """å¤§å°çƒç‰¹å¾ - å›ºå®š12ç»´"""
        if not daxiao_odds or len(daxiao_odds) == 0:
            return [0.0] * 12

        try:
            df = pd.DataFrame(daxiao_odds)
            features = []

            for col in ['live_over', 'live_under']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()), 
                        float(df[col].std()) if len(df) > 1 else 0.0, 
                        float(df[col].min()), 
                        float(df[col].max())
                    ])
                else:
                    features.extend([0.0] * 4)

            if 'live_line' in df.columns:
                lines = []
                for line in df['live_line']:
                    try:
                        line_str = str(line).replace('çƒ', '').replace('åŠ', '.5')
                        lines.append(float(line_str))
                    except:
                        lines.append(2.5)

                if len(lines) > 0:
                    features.extend([
                        float(np.mean(lines)),
                        float(np.std(lines)) if len(lines) > 1 else 0.0,
                        float(max(lines)),
                        float(min(lines))
                    ])
                else:
                    features.extend([0.0] * 4)
            else:
                features.extend([0.0] * 4)

            return features[:12]
        except Exception as e:
            return [0.0] * 12 * 12

    def _meta_features(self, match_data):
        """å…ƒç‰¹å¾ - å›ºå®š13ç»´"""
        features = []

        league = match_data.get('league', '')
        tier = 3
        if any(x in league for x in ['è‹±è¶…','è¥¿ç”²','æ„ç”²','å¾·ç”²','æ³•ç”²','æ¬§å† ']): tier = 1
        elif any(x in league for x in ['è·ç”²','è‘¡è¶…','ä¿„è¶…','æ¯”ç”²','æ¬§ç½—å·´']): tier = 2
        features.append(float(tier))

        match_time = match_data.get('time', '15:00')
        try:
            hour = int(match_time.split(':')[0])
            features.extend([
                float(np.sin(2 * np.pi * hour / 24)),
                float(np.cos(2 * np.pi * hour / 24)),
                1.0 if 19 <= hour <= 23 else 0.0,
                1.0 if hour < 12 else 0.0
            ])
        except:
            features.extend([0.0, 0.0, 0.0, 0.0])

        europe = match_data.get('europe', [])
        if europe and len(europe) > 0:
            try:
                df = pd.DataFrame(europe)
                if 'live_home' in df.columns and 'live_away' in df.columns:
                    avg_home = df['live_home'].mean()
                    avg_away = df['live_away'].mean()
                    features.extend([
                        float(1/avg_home) if avg_home > 0 else 0.5,
                        float(1/avg_away) if avg_away > 0 else 0.5,
                        float(avg_away / avg_home) if avg_home > 0 else 1.0,
                        float((avg_home < avg_away).sum() / len(df)) if len(df) > 0 else 0.5
                    ])
                else:
                    features.extend([0.5, 0.5, 1.0, 0.5])
            except:
                features.extend([0.5, 0.5, 1.0, 0.5])
        else:
            features.extend([0.5, 0.5, 1.0, 0.5])

        for odds_type in ['europe', 'asia', 'handicap', 'daxiao']:
            odds = match_data.get(odds_type, [])
            features.append(float(len(odds)) if odds else 0.0)

        return features[:13]

    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List]:
        """å‡†å¤‡è®­ç»ƒæ•°æ® - å¢å¼ºç‰ˆï¼Œå¤„ç†ç±»åˆ«ä¸å¹³è¡¡"""
        X, y_labels, metadata = [], [], []

        # è°ƒè¯•ç»Ÿè®¡
        total_rows = len(df)
        has_result_count = 0
        has_odds_count = 0
        valid_features_count = 0

        for _, row in df.iterrows():
            if not row.get('actual_result') or row['actual_result'] not in ['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ']:
                continue
            has_result_count += 1

            has_odds = any(len(row.get(ot, []) or []) > 0 for ot in ['europe', 'asia', 'handicap', 'daxiao'])
            if not has_odds:
                continue
            has_odds_count += 1

            features = self.extract_features(row.to_dict())
            valid_features_count += 1

            # ç¡®ä¿ç»´åº¦
            if len(features) != self.FEATURE_DIM:
                if len(features) > self.FEATURE_DIM:
                    features = features[:self.FEATURE_DIM]
                else:
                    features = np.concatenate([features, np.zeros(self.FEATURE_DIM - len(features))])

            X.append(features)

            result = row['actual_result']
            if result == 'ä¸»èƒœ':
                y_labels.append(0)
            elif result == 'å¹³å±€':
                y_labels.append(1)
            else:
                y_labels.append(2)

            metadata.append({
                'match_id': row['match_id'],
                'teams': f"{row['home_team']} vs {row['away_team']}",
                'date': row['date'],
                'league': row['league']
            })

        # è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        print(f"ğŸ“Š æ•°æ®è¿‡æ»¤ç»Ÿè®¡:")
        print(f"   æ€»è¡Œæ•°: {total_rows}")
        print(f"   æœ‰ç»“æœ: {has_result_count}")
        print(f"   æœ‰èµ”ç‡: {has_odds_count}")
        print(f"   æœ‰æ•ˆç‰¹å¾: {valid_features_count}")

        if len(X) == 0:
            return np.array([]), np.array([]), []

        X = np.array(X)
        y_labels = np.array(y_labels)

        # æ£€æŸ¥ç±»åˆ«æ•°é‡
        unique_classes = np.unique(y_labels)
        class_counts = np.bincount(y_labels, minlength=3)
        print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: ä¸»èƒœ={class_counts[0]}, å¹³å±€={class_counts[1]}, å®¢èƒœ={class_counts[2]}")

        if len(unique_classes) < 2:
            print(f"âš ï¸ åªæœ‰ {len(unique_classes)} ä¸ªç±»åˆ«ï¼Œæ— æ³•è®­ç»ƒ")
            return np.array([]), np.array([]), []

        # æ•°æ®å¢å¼º
        min_count = min(class_counts[class_counts > 0])
        if min_count < 5:
            print(f"âš ï¸ æŸäº›ç±»åˆ«æ ·æœ¬è¿‡å°‘ï¼Œæ­£åœ¨æ•°æ®å¢å¼º...")
            X_new, y_new, meta_new = [], [], []

            for cls in range(3):
                mask = y_labels == cls
                X_cls = X[mask]
                y_cls = y_labels[mask]
                meta_cls = [metadata[i] for i in range(len(metadata)) if mask[i]]

                if len(X_cls) > 0 and len(X_cls) < 5:
                    repeat_times = (5 // len(X_cls)) + 1
                    X_cls = np.repeat(X_cls, repeat_times, axis=0)[:10]
                    y_cls = np.repeat(y_cls, repeat_times)[:10]
                    meta_cls = (meta_cls * repeat_times)[:10]

                if len(X_cls) > 0:
                    X_new.extend(X_cls)
                    y_new.extend(y_cls)
                    meta_new.extend(meta_cls)

            X = np.array(X_new)
            y_labels = np.array(y_new)
            metadata = meta_new
            print(f"ğŸ“ˆ å¢å¼ºå: {len(X)} ä¸ªæ ·æœ¬")

        # è½¬æ¢ä¸ºone-hot
        y_onehot = np.zeros((len(y_labels), 3))
        for i, label in enumerate(y_labels):
            y_onehot[i, label] = 1

        if not self.is_fitted:
            X = self.scaler.fit_transform(X)
            self.is_fitted = True
        else:
            X = self.scaler.transform(X)

        return X, y_onehot, metadata

    def transform(self, match_data: Dict) -> np.ndarray:
        features = self.extract_features(match_data)

        if len(features) != self.FEATURE_DIM:
            if len(features) > self.FEATURE_DIM:
                features = features[:self.FEATURE_DIM]
            else:
                features = np.concatenate([features, np.zeros(self.FEATURE_DIM - len(features))])

        if self.is_fitted:
            features = self.scaler.transform(features.reshape(1, -1))[0]
        return features

    def save(self, path):
        with open(path, 'wb') as f:
            pickle.dump({'scaler': self.scaler, 'is_fitted': self.is_fitted}, f)

    def load(self, path):
        with open(path, 'rb') as f:
            data = pickle.load(f)
            self.scaler = data['scaler']
            self.is_fitted = data['is_fitted']


class DeepLearningModel:
    """æ·±åº¦å­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, input_dim=85):
        self.input_dim = input_dim
        self.dnn_model = None
        self.rf_model = None
        self.gbdt_model = None
        self.is_trained = False
        self.training_history = []
    
    def build_models(self):
        if TF_AVAILABLE:
            self.dnn_model = self._build_dnn()
        
        self.rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, random_state=42, n_jobs=-1)
        self.gbdt_model = GradientBoostingClassifier(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=42)
    
    def _build_dnn(self):
        model = Sequential([
            Dense(256, activation='relu', input_shape=(self.input_dim,)),
            BatchNormalization(),
            Dropout(0.4),
            Dense(128, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),
            Dense(3, activation='softmax')
        ])
        
        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
        return model
    
    def train(self, X, y, validation_split=0.2, epochs=100):
        if len(X) < 10:
            return False, "æ•°æ®é‡ä¸è¶³"
        
        results = {}
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, random_state=42)
        
        if TF_AVAILABLE and self.dnn_model:
            callbacks = [
                EarlyStopping(patience=15, restore_best_weights=True, monitor='val_accuracy'),
                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)
            ]
            history = self.dnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), 
                                        epochs=epochs, batch_size=32, callbacks=callbacks, verbose=0)
            results['dnn'] = {
                'train_acc': max(history.history['accuracy']),
                'val_acc': max(history.history['val_accuracy'])
            }
            self.training_history.append(history.history)
        
        y_train_labels = np.argmax(y_train, axis=1)
        y_val_labels = np.argmax(y_val, axis=1)
        
        self.rf_model.fit(X_train, y_train_labels)
        results['rf'] = {
            'train_acc': self.rf_model.score(X_train, y_train_labels),
            'val_acc': self.rf_model.score(X_val, y_val_labels)
        }
        
        self.gbdt_model.fit(X_train, y_train_labels)
        results['gbdt'] = {
            'train_acc': self.gbdt_model.score(X_train, y_train_labels),
            'val_acc': self.gbdt_model.score(X_val, y_val_labels)
        }
        
        self.is_trained = True
        return True, results
    
    def predict(self, X):
        if not self.is_trained:
            return None
        
        dnn_pred = None
        if TF_AVAILABLE and self.dnn_model:
            dnn_pred = self.dnn_model.predict(X.reshape(1, -1), verbose=0)[0]
        
        rf_pred = self.rf_model.predict_proba(X.reshape(1, -1))[0]
        gbdt_pred = self.gbdt_model.predict_proba(X.reshape(1, -1))[0]
        
        if dnn_pred is not None:
            ensemble_pred = 0.4 * dnn_pred + 0.3 * rf_pred + 0.3 * gbdt_pred
        else:
            ensemble_pred = 0.5 * rf_pred + 0.5 * gbdt_pred
        
        ensemble_pred = ensemble_pred / ensemble_pred.sum()
        
        labels = ['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ']
        pred_idx = np.argmax(ensemble_pred)
        
        return {
            'result': labels[pred_idx],
            'confidence': round(ensemble_pred[pred_idx] * 100, 2),
            'probabilities': {labels[i]: round(ensemble_pred[i] * 100, 2) for i in range(3)}
        }
    
    def _predict_scores(self, home_win_prob, draw_prob, away_win_prob):
        """é¢„æµ‹æœ€å¯èƒ½çš„3ä¸ªæ¯”åˆ†"""
        scores = []

        if home_win_prob > 0.3:
            scores.extend([
                ('1-0', home_win_prob * 0.25),
                ('2-0', home_win_prob * 0.20),
                ('2-1', home_win_prob * 0.18),
                ('3-0', home_win_prob * 0.12),
                ('3-1', home_win_prob * 0.10),
            ])

        if draw_prob > 0.2:
            scores.extend([
                ('1-1', draw_prob * 0.40),
                ('0-0', draw_prob * 0.30),
                ('2-2', draw_prob * 0.20),
            ])

        if away_win_prob > 0.3:
            scores.extend([
                ('0-1', away_win_prob * 0.25),
                ('0-2', away_win_prob * 0.20),
                ('1-2', away_win_prob * 0.18),
                ('0-3', away_win_prob * 0.12),
                ('1-3', away_win_prob * 0.10),
            ])

        score_dict = {}
        for score, weight in scores:
            if score in score_dict:
                score_dict[score] += weight
            else:
                score_dict[score] = weight

        sorted_scores = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
        top3 = sorted_scores[:3]

        total_weight = sum(w for _, w in top3) if top3 else 1
        return [(score, round(w/total_weight*100, 2)) for score, w in top3]

    def _predict_total_goals(self, home_win_prob, draw_prob, away_win_prob):
        """é¢„æµ‹æœ€å¯èƒ½çš„3ä¸ªæ€»è¿›çƒæ•°"""
        goals = []

        goals.extend([
            (0, draw_prob * 0.15 + (home_win_prob + away_win_prob) * 0.05),
            (1, (home_win_prob + away_win_prob) * 0.25),
            (2, draw_prob * 0.30 + (home_win_prob + away_win_prob) * 0.20),
            (3, draw_prob * 0.35 + (home_win_prob + away_win_prob) * 0.30),
            (4, draw_prob * 0.15 + (home_win_prob + away_win_prob) * 0.25),
            (5, (home_win_prob + away_win_prob) * 0.10),
            (6, (home_win_prob + away_win_prob) * 0.05),
        ])

        sorted_goals = sorted(goals, key=lambda x: x[1], reverse=True)
        top3 = sorted_goals[:3]

        total_weight = sum(w for _, w in top3) if top3 else 1
        return [(str(goals), round(w/total_weight*100, 2)) for goals, w in top3]

    def save(self, name='model_v5'):
        path = os.path.join(CONFIG.MODEL_DIR, name)
        os.makedirs(path, exist_ok=True)
        
        if TF_AVAILABLE and self.dnn_model:
            self.dnn_model.save(os.path.join(path, 'dnn.h5'))
        
        with open(os.path.join(path, 'rf.pkl'), 'wb') as f:
            pickle.dump(self.rf_model, f)
        with open(os.path.join(path, 'gbdt.pkl'), 'wb') as f:
            pickle.dump(self.gbdt_model, f)
        
        with open(os.path.join(path, 'config.pkl'), 'wb') as f:
            pickle.dump({'is_trained': self.is_trained, 'training_history': self.training_history}, f)
        
        return True
    
    def load(self, name='model_v5'):
        path = os.path.join(CONFIG.MODEL_DIR, name)
        
        if not os.path.exists(path):
            return False
        
        try:
            if TF_AVAILABLE and os.path.exists(os.path.join(path, 'dnn.h5')):
                self.dnn_model = load_model(os.path.join(path, 'dnn.h5'))
            
            with open(os.path.join(path, 'rf.pkl'), 'rb') as f:
                self.rf_model = pickle.load(f)
            with open(os.path.join(path, 'gbdt.pkl'), 'rb') as f:
                self.gbdt_model = pickle.load(f)
            
            with open(os.path.join(path, 'config.pkl'), 'rb') as f:
                config = pickle.load(f)
                self.is_trained = config['is_trained']
                self.training_history = config['training_history']
            
            return True
        except:
            return False

# ==================== ä¸»æ§åˆ¶ç³»ç»Ÿ ====================

class FootballPredictionSystem:
    """è¶³çƒé¢„æµ‹ä¸»ç³»ç»Ÿ"""
    
    def __init__(self):
        self.collector = DataCollector()
        self.feature_engineer = FeatureEngineer()
        self.model = DeepLearningModel(input_dim=FeatureEngineer.FEATURE_DIM)
        self.data_file = CONFIG.DATA_FILE
        
        self.df = self._load_data()
        
        if self.model.load():
            scaler_path = os.path.join(CONFIG.MODEL_DIR, 'scaler.pkl')
            if os.path.exists(scaler_path):
                self.feature_engineer.load(scaler_path)
    
    def _load_data(self):
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                df = pd.DataFrame(data)
                
                required_cols = ['match_id', 'date', 'league', 'time', 'home_team', 'away_team', 'actual_result']
                for col in required_cols:
                    if col not in df.columns:
                        df[col] = ''
                
                for col in ['europe', 'asia', 'daxiao', 'handicap']:
                    if col in df.columns:
                        df[col] = df[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x if isinstance(x, list) else [])
                    else:
                        df[col] = [[] for _ in range(len(df))]
                
                return df
            except Exception as e:
                print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        
        return pd.DataFrame(columns=[
            'match_id', 'date', 'league', 'time', 'status', 'home_team', 'away_team',
            'score', 'score_home', 'score_away', 'actual_result', 'has_result',
            'europe', 'asia', 'daxiao', 'handicap', 'order'
        ])
    
    def _save_data(self):
        try:
            if self.df.empty:
                with open(self.data_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                return True
            
            df_copy = self.df.copy()
            for col in ['europe', 'asia', 'daxiao', 'handicap']:
                if col in df_copy.columns:
                    def serialize_odds(x):
                        if isinstance(x, list):
                            return json.dumps(x, ensure_ascii=False)
                        elif isinstance(x, str):
                            try:
                                json.loads(x)
                                return x
                            except:
                                return json.dumps([], ensure_ascii=False)
                        else:
                            return json.dumps([], ensure_ascii=False)

                    df_copy[col] = df_copy[col].apply(serialize_odds)
            
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(df_copy.to_dict('records'), f, ensure_ascii=False, indent=2)

            # è°ƒè¯•ä¿¡æ¯
            print(f"ğŸ’¾ ä¿å­˜äº† {len(df_copy)} åœºæ¯”èµ›æ•°æ®")
            for col in ['europe', 'asia', 'daxiao', 'handicap']:
                if col in df_copy.columns:
                    count = df_copy[col].apply(lambda x: len(json.loads(x)) > 0 if isinstance(x, str) else False).sum()
                    print(f"   {col}: {count} åœºæœ‰æ•°æ®")

            return True
        except Exception as e:
            print(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def batch_collect_training_data(self, start_date: str, days: int = 7, progress_callback=None, log_callback=None):
        """æ‰¹é‡æ”¶é›†è®­ç»ƒæ•°æ®"""
        new_df = self.collector.batch_fetch_history(start_date, days, progress_callback, log_callback)
        
        if new_df.empty:
            return False, "æœªè·å–åˆ°æ•°æ®"
        
        if not self.df.empty and 'match_id' in self.df.columns:
            existing_ids = set(self.df['match_id'].tolist())
            new_df = new_df[~new_df['match_id'].isin(existing_ids)]
        
        if not new_df.empty:
            for col in self.df.columns:
                if col not in new_df.columns:
                    if col in ['europe', 'asia', 'daxiao', 'handicap']:
                        new_df[col] = [[] for _ in range(len(new_df))]
                    else:
                        new_df[col] = ['' for _ in range(len(new_df))]
            
            for col in new_df.columns:
                if col not in self.df.columns:
                    if col in ['europe', 'asia', 'daxiao', 'handicap']:
                        self.df[col] = [[] for _ in range(len(self.df))]
                    else:
                        self.df[col] = ['' for _ in range(len(self.df))]
            
            self.df = pd.concat([self.df, new_df], ignore_index=True)
            self._save_data()
            
            has_result = len(new_df[new_df['actual_result'] != ''])
            has_odds = len(new_df[new_df['europe'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)])
            
            return True, f"æ–°å¢ {len(new_df)} åœºï¼ˆæœ‰ç»“æœ: {has_result}åœºï¼Œæœ‰èµ”ç‡: {has_odds}åœºï¼‰ï¼Œç´¯è®¡ {len(self.df)} åœº"
        
        return True, "æ‰€æœ‰æ•°æ®å·²æ˜¯æœ€æ–°"

    def collect_future_matches(self, date_str: str, progress_callback=None, log_callback=None):
        """æ”¶é›†æœªæ¥æ¯”èµ›ç”¨äºé¢„æµ‹"""
        new_df = self.collector.fetch_future_matches_with_odds(date_str, progress_callback, log_callback)

        if new_df.empty:
            return False, "æœªè·å–åˆ°æœªæ¥æ¯”èµ›"

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if not self.df.empty and 'match_id' in self.df.columns:
            existing_ids = set(self.df['match_id'].tolist())
            new_df = new_df[~new_df['match_id'].isin(existing_ids)]

        if not new_df.empty:
            # ç¡®ä¿åˆ—ä¸€è‡´
            for col in self.df.columns:
                if col not in new_df.columns:
                    if col in ['europe', 'asia', 'daxiao', 'handicap']:
                        new_df[col] = [[] for _ in range(len(new_df))]
                    else:
                        new_df[col] = ['' for _ in range(len(new_df))]

            for col in new_df.columns:
                if col not in self.df.columns:
                    if col in ['europe', 'asia', 'daxiao', 'handicap']:
                        self.df[col] = [[] for _ in range(len(self.df))]
                    else:
                        self.df[col] = ['' for _ in range(len(self.df))]

            self.df = pd.concat([self.df, new_df], ignore_index=True)
            self._save_data()

            has_odds = len(new_df[new_df['europe'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)])

            return True, f"æ–°å¢ {len(new_df)} åœºæœªæ¥æ¯”èµ›ï¼ˆæœ‰èµ”ç‡: {has_odds}åœºï¼‰"

        return True, "æ‰€æœ‰æœªæ¥æ¯”èµ›å·²æ˜¯æœ€æ–°"

    def train_model(self):
        """è®­ç»ƒæ¨¡å‹"""
        if self.df.empty or 'actual_result' not in self.df.columns:
            return False, "æ²¡æœ‰è®­ç»ƒæ•°æ®"
        
        train_df = self.df[self.df['actual_result'].isin(['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ'])].copy()
        
        if len(train_df) < CONFIG.MIN_TRAIN_SAMPLES:
            return False, f"éœ€è¦è‡³å°‘{CONFIG.MIN_TRAIN_SAMPLES}åœºæœ‰ç»“æœçš„æ¯”èµ›ï¼Œå½“å‰{len(train_df)}åœº"
        
        has_odds_count = 0
        for idx, row in train_df.iterrows():
            for odds_type in ['europe', 'asia', 'handicap', 'daxiao']:
                odds = row.get(odds_type, [])
                if odds and len(odds) > 0:
                    has_odds_count += 1
                    break
        
        if has_odds_count == 0:
            return False, f"æœ‰ç»“æœçš„æ¯”èµ›: {len(train_df)}åœºï¼Œä½†æœ‰èµ”ç‡æ•°æ®çš„: 0åœºã€‚è¯·ç¡®ä¿æˆåŠŸè·å–äº†èµ”ç‡ä¿¡æ¯ã€‚"
        
        X, y, metadata = self.feature_engineer.prepare_training_data(train_df)
        
        if len(X) == 0:
            return False, f"ç‰¹å¾æå–å¤±è´¥ã€‚æœ‰èµ”ç‡çš„æ¯”èµ›: {has_odds_count}åœºï¼Œä½†æ— æ³•æå–æœ‰æ•ˆç‰¹å¾ã€‚"
        
        self.model.build_models()
        success, results = self.model.train(X, y)
        
        if success:
            self.model.save()
            self.feature_engineer.save(os.path.join(CONFIG.MODEL_DIR, 'scaler.pkl'))
            return True, results
        
        return False, results
    
    def predict(self, match_data):
        """é¢„æµ‹"""
        if not self.model.is_trained:
            return None, "æ¨¡å‹æœªè®­ç»ƒ"
        
        features = self.feature_engineer.transform(match_data)
        result = self.model.predict(features)
        return result, None
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡"""
        if self.df.empty:
            return {'total': 0, 'trainable': 0, 'model_ready': self.model.is_trained}
        
        if 'actual_result' not in self.df.columns:
            trainable = 0
        else:
            trainable = len(self.df[self.df['actual_result'].isin(['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ'])])
        
        return {
            'total': len(self.df),
            'trainable': trainable,
            'model_ready': self.model.is_trained
        }

# ==================== Streamlit UI ====================

def main():
    st.set_page_config(page_title="âš½ è¶³çƒæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ v5.2", layout="wide")
    
    st.markdown("""
    <style>
    .main-header { font-size: 2.5rem; font-weight: bold; color: #1f77b4; text-align: center; }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<div class="main-header">âš½ è¶³çƒæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ v5.2<br><small>ä¼˜åŒ–èµ”ç‡æå–ç‰ˆ</small></div>', unsafe_allow_html=True)
    
    if 'system' not in st.session_state:
        with st.spinner("ç³»ç»Ÿåˆå§‹åŒ–ä¸­..."):
            st.session_state['system'] = FootballPredictionSystem()
    
    system = st.session_state['system']
    stats = system.get_stats()
    
    with st.sidebar:
        st.header("ğŸ›ï¸ æ§åˆ¶é¢æ¿")
        
        st.subheader("ğŸ“š æ‰¹é‡è·å–è®­ç»ƒæ•°æ®")
        col_date, col_days = st.columns([2, 1])
        with col_date:
            start_date = st.date_input("å¼€å§‹æ—¥æœŸ", datetime.now() - timedelta(days=7))
        with col_days:
            days = st.number_input("å¤©æ•°", min_value=1, max_value=30, value=7)
        
        if 'fetch_logs' not in st.session_state:
            st.session_state['fetch_logs'] = []
        
        log_area = st.empty()
        if st.session_state['fetch_logs']:
            with log_area.container():
                st.markdown("ğŸ“‹ **è·å–æ—¥å¿—**")
                log_html = "<div style='height: 120px; overflow-y: auto; border: 1px solid #e0e0e0; border-radius: 5px; padding: 8px; background-color: #f8f9fa; font-family: monospace; font-size: 10px; line-height: 1.6;'>"
                for log in st.session_state['fetch_logs'][-50:]:
                    color = '#0066cc' if 'ğŸ“¡' in log else '#28a745' if 'âœ…' in log else '#333'
                    log_html += f"<div style='color: {color}; margin-bottom: 2px;'>{log}</div>"
                log_html += "</div>"
                st.markdown(log_html, unsafe_allow_html=True)
        
        # è·å–æœªæ¥æ¯”èµ›
        st.subheader("è·å–æœªæ¥æ¯”èµ›")
        future_date = st.date_input("é€‰æ‹©æ¯”èµ›æ—¥æœŸ", datetime.now() + timedelta(days=1))

        if st.button("è·å–æœªæ¥æ¯”èµ›", use_container_width=True, type="primary"):
            st.session_state['fetch_logs'] = []

            progress_bar = st.progress(0)
            status_text = st.empty()

            def update_progress(current, total, date_str, status):
                progress_bar.progress(current / total if total > 0 else 0)
                status_text.text(f"[{current}/{total}] {status}")

            def add_log(message):
                st.session_state['fetch_logs'].append(message)
                if len(st.session_state['fetch_logs']) > 100:
                    st.session_state['fetch_logs'] = st.session_state['fetch_logs'][-100:]

            with st.spinner("è·å–æœªæ¥æ¯”èµ›ä¸­..."):
                success, msg = system.collect_future_matches(
                    future_date.strftime('%Y-%m-%d'),
                    update_progress,
                    add_log
                )

            progress_bar.empty()
            status_text.empty()

            if success:
                st.success(msg)
                st.balloons()
            else:
                st.error(msg)

            st.rerun()

        st.markdown("---")

        if st.button("ğŸš€ æ‰¹é‡è·å–è®­ç»ƒæ•°æ®", use_container_width=True, type="primary"):
            st.session_state['fetch_logs'] = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current_day, total_days, date_str, status):
                progress_bar.progress(current_day / total_days)
                status_text.text(f"[{current_day}/{total_days}] {date_str}: {status}")
            
            def add_log(message):
                st.session_state['fetch_logs'].append(message)
                if len(st.session_state['fetch_logs']) > 100:
                    st.session_state['fetch_logs'] = st.session_state['fetch_logs'][-100:]
            
            with st.spinner("è·å–ä¸­..."):
                success, msg = system.batch_collect_training_data(
                    start_date.strftime('%Y-%m-%d'),
                    int(days),
                    update_progress,
                    add_log
                )
            
            progress_bar.empty()
            status_text.empty()
            
            if success:
                st.success(msg)
                st.balloons()
            else:
                st.error(msg)
            
            st.rerun()
        
        st.markdown("---")
        
        st.subheader("ğŸ§  æ¨¡å‹è®­ç»ƒ")
        trainable_count = stats['trainable']
        
        if trainable_count >= CONFIG.MIN_TRAIN_SAMPLES:
            st.success(f"âœ… å¯è®­ç»ƒæ•°æ®: {trainable_count}åœº")
            if st.button("å¼€å§‹è®­ç»ƒæ¨¡å‹", use_container_width=True, type="primary"):
                with st.spinner("è®­ç»ƒä¸­..."):
                    success, result = system.train_model()
                    if success:
                        st.success("è®­ç»ƒå®Œæˆï¼")
                        st.json(result)
                    else:
                        st.error(result)
        else:
            st.warning(f"âš ï¸ éœ€è¦{CONFIG.MIN_TRAIN_SAMPLES}åœºï¼Œå½“å‰{trainable_count}åœº")
            st.progress(trainable_count / CONFIG.MIN_TRAIN_SAMPLES if CONFIG.MIN_TRAIN_SAMPLES > 0 else 0)
        
        st.markdown("---")
        
        if st.button("ğŸ’¾ ä¿å­˜æ•°æ®", use_container_width=True):
            if system._save_data():
                st.success("å·²ä¿å­˜")
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ•°æ®", use_container_width=True):
            system.df = pd.DataFrame(columns=[
                'match_id', 'date', 'league', 'time', 'status', 'home_team', 'away_team',
                'score', 'score_home', 'score_away', 'actual_result', 'has_result',
                'europe', 'asia', 'daxiao', 'handicap', 'order'
            ])
            system._save_data()
            st.success("å·²æ¸…ç©º")
    
    tabs = st.tabs(["ğŸ“Š æ•°æ®æ¦‚è§ˆ", "ğŸ¯ é¢„æµ‹ä¸­å¿ƒ"])
    
    with tabs[0]:
        st.subheader("è®­ç»ƒæ•°æ®æ¦‚è§ˆ")
        
        cols = st.columns(4)
        with cols[0]:
            st.metric("æ€»æ¯”èµ›æ•°", stats['total'])
        with cols[1]:
            st.metric("å¯è®­ç»ƒæ•°æ®", stats['trainable'])
        with cols[2]:
            st.metric("è®­ç»ƒé—¨æ§›", CONFIG.MIN_TRAIN_SAMPLES)
        with cols[3]:
            st.metric("æ¨¡å‹çŠ¶æ€", "âœ… å°±ç»ª" if stats['model_ready'] else "âŒ æœªè®­ç»ƒ")
        
        if not system.df.empty:
            st.markdown("---")
            
            display_df = system.df.copy()
            if 'actual_result' in display_df.columns:
                display_df = display_df[display_df['actual_result'].isin(['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ'])]
            
            display_cols = ['date', 'league', 'home_team', 'away_team', 'score', 'actual_result']
            display_cols = [c for c in display_cols if c in display_df.columns]

            # æ·»åŠ èµ”ç‡çŠ¶æ€åˆ—
            def get_odds_status(row):
                odds_types = []
                for ot in ['europe', 'asia', 'handicap', 'daxiao']:
                    if ot in row and isinstance(row[ot], list) and len(row[ot]) > 0:
                        odds_types.append(ot[:2])
                return ','.join(odds_types) if odds_types else 'æ— '

            if not display_df.empty:
                display_df['odds'] = display_df.apply(get_odds_status, axis=1)
                display_cols.append('odds')
            
            if not display_df.empty:
                if 'order' in display_df.columns:
                    display_df = display_df.sort_values(['date', 'order'])
                
                st.dataframe(display_df[display_cols], use_container_width=True, height=400)
                
                st.markdown("---")
                st.subheader("èµ”ç‡æ•°æ®ç»Ÿè®¡")
                odds_stats = {}
                for odds_type in ['europe', 'asia', 'handicap', 'daxiao']:
                    if odds_type in display_df.columns:
                        count = display_df[odds_type].apply(lambda x: len(x) > 0 if isinstance(x, list) else False).sum()
                        odds_stats[odds_type] = count
                
                if odds_stats:
                    st.write(odds_stats)
            else:
                st.info("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
        else:
            st.info("æš‚æ— æ•°æ®ï¼Œè¯·ä½¿ç”¨ä¾§è¾¹æ è·å–è®­ç»ƒæ•°æ®")
    
    with tabs[1]:
        st.subheader("æ¯”èµ›é¢„æµ‹")
        
        if not system.df.empty and stats['model_ready']:
            future_matches = system.df[system.df['actual_result'] == ''] if 'actual_result' in system.df.columns else system.df
            
            if not future_matches.empty:
                selected = st.selectbox(
                    "é€‰æ‹©æ¯”èµ›è¿›è¡Œé¢„æµ‹",
                    future_matches.to_dict('records'),
                    format_func=lambda x: f"{x.get('date', 'N/A')} | {x.get('league', 'N/A')} | {x.get('home_team', 'N/A')} vs {x.get('away_team', 'N/A')}"
                )
                
                if selected:
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown(f"### {selected.get('home_team', 'N/A')} ğŸ†š {selected.get('away_team', 'N/A')}")
                        st.write(f"**è”èµ›:** {selected.get('league', 'N/A')}")
                        
                        europe_odds = selected.get('europe', [])
                        has_odds = europe_odds and len(europe_odds) > 0
                        if has_odds:
                            st.write(f"**èµ”ç‡æ•°æ®:** âœ… å·²è·å– ({len(europe_odds)}å®¶å…¬å¸)")
                        else:
                            st.warning("âš ï¸ æš‚æ— èµ”ç‡æ•°æ®")
                    
                    with col2:
                        if has_odds:
                            if st.button("ğŸ”® å¼€å§‹é¢„æµ‹", type="primary", use_container_width=True):
                                result, error = system.predict(selected)
                                if error:
                                    st.error(error)
                                else:
                                    # èƒœå¹³è´Ÿé¢„æµ‹
                                    st.success(f"**é¢„æµ‹ç»“æœ: {result['result']}**")
                                    st.write(f"**ç½®ä¿¡åº¦:** {result['confidence']}%")

                                    # æ˜¾ç¤ºå‰3ä¸ªæœ€å¯èƒ½çš„ç»“æœ
                                    st.markdown("**èƒœå¹³è´Ÿæ¦‚ç‡ Top 3:**")
                                    for i, (res, conf) in enumerate(result['top3_results'], 1):
                                        st.write(f"{i}. {res}: {conf}%")

                                    # æ¯”åˆ†é¢„æµ‹
                                    st.markdown("**æœ€å¯èƒ½æ¯”åˆ† Top 3:**")
                                    for i, (score, prob) in enumerate(result['score_predictions'], 1):
                                        st.write(f"{i}. {score}: {prob}%")

                                    # æ€»è¿›çƒæ•°é¢„æµ‹
                                    st.markdown("**æ€»è¿›çƒæ•° Top 3:**")
                                    for i, (goals, prob) in enumerate(result['total_goals_predictions'], 1):
                                        st.write(f"{i}. {goals}çƒ: {prob}%")
                        else:
                            st.info("è¯·å…ˆè·å–èµ”ç‡æ•°æ®")
            else:
                st.info("å½“å‰æ²¡æœ‰æœªé¢„æµ‹çš„æ¯”èµ›")
        else:
            if not stats['model_ready']:
                st.warning("âš ï¸ æ¨¡å‹æœªè®­ç»ƒ")
            else:
                st.info("æš‚æ— æ¯”èµ›æ•°æ®")

if __name__ == "__main__":
    main()