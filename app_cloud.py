# ==================== è¶³çƒæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ v5.2 (Cloud Edition) ====================
# ä¼˜åŒ–ï¼šé€‚é… Streamlit Cloudï¼Œä¿®å¤ Selenium é…ç½®

import streamlit as st
import requests
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import re
import time
import json
import os
import pickle
from collections import defaultdict
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ 
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential, load_model
    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.optimizers import Adam
    TF_AVAILABLE = True
except ImportError:
    TF_AVAILABLE = False

# æœºå™¨å­¦ä¹ 
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# çˆ¬è™«
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup

# å¯è§†åŒ–
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# ==================== é…ç½® ====================

@dataclass
class Config:
    DATA_FILE: str = "football_training_data.json"
    MODEL_DIR: str = "models_v5"
    MIN_TRAIN_SAMPLES: int = 30
    
    def __post_init__(self):
        os.makedirs(self.MODEL_DIR, exist_ok=True)

CONFIG = Config()

# ==================== æ•°æ®æŒä¹…åŒ–ç®¡ç† ====================

class DataPersistence:
    """æ•°æ®æŒä¹…åŒ–ç®¡ç†å™¨ - è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°"""

    DATA_FILE = "football_data_cache.json"

    def __init__(self):
        self.data = self._load_data()

    def _load_data(self):
        """ä»æœ¬åœ°åŠ è½½æ•°æ®"""
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                print(f"ğŸ“‚ ä»æœ¬åœ°åŠ è½½äº† {len(data)} åœºæ¯”èµ›æ•°æ®")

                # è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame(data)

                # è§£æJSONå­—ç¬¦ä¸²ä¸ºåˆ—è¡¨
                for col in ['europe', 'asia', 'daxiao', 'handicap']:
                    if col in df.columns:
                        df[col] = df[col].apply(
                            lambda x: json.loads(x) if isinstance(x, str) and x.strip() else 
                                     (x if isinstance(x, list) else [])
                        )

                # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
                required_cols = ['match_id', 'date', 'league', 'time', 'home_team', 'away_team', 'actual_result']
                for col in required_cols:
                    if col not in df.columns:
                        df[col] = ''

                return df
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æœ¬åœ°æ•°æ®å¤±è´¥: {e}")

        # è¿”å›ç©ºDataFrame
        return pd.DataFrame(columns=[
            'match_id', 'date', 'league', 'time', 'status', 'home_team', 'away_team',
            'score', 'score_home', 'score_away', 'actual_result', 'has_result',
            'europe', 'asia', 'daxiao', 'handicap', 'order'
        ])

    def save_data(self):
        """ä¿å­˜æ•°æ®åˆ°æœ¬åœ°"""
        try:
            with open(self.DATA_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False

    def add_matches(self, matches):
        """æ‰¹é‡æ·»åŠ æ¯”èµ›"""
        added, updated = 0, 0
        for match in matches:
            match_id = match.get('match_id')
            existing_idx = None
            for idx, m in enumerate(self.data):
                if m.get('match_id') == match_id:
                    existing_idx = idx
                    break

            if existing_idx is not None:
                existing = self.data[existing_idx]
                for key in ['europe', 'asia', 'handicap', 'daxiao']:
                    if key in match and match[key]:
                        existing[key] = match[key]
                for key, value in match.items():
                    if key not in ['europe', 'asia', 'handicap', 'daxiao']:
                        existing[key] = value
                self.data[existing_idx] = existing
                updated += 1
            else:
                self.data.append(match)
                added += 1

        self.save_data()
        return added, updated

    def get_trainable_matches(self):
        """è·å–å¯ç”¨äºè®­ç»ƒçš„æ¯”èµ›"""
        result = []
        for m in self.data:
            has_result = m.get('actual_result') in ['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ']
            has_odds = any(len(m.get(ot, []) or []) > 0 for ot in ['europe', 'asia', 'handicap', 'daxiao'])
            if has_result and has_odds:
                result.append(m)
        return result

    def get_statistics(self):
        """è·å–ç»Ÿè®¡"""
        trainable = self.get_trainable_matches()
        result_dist = {"ä¸»èƒœ": 0, "å¹³å±€": 0, "å®¢èƒœ": 0}
        for m in trainable:
            r = m.get('actual_result')
            if r in result_dist:
                result_dist[r] += 1
        return {
            'total': len(self.data),
            'trainable': len(trainable),
            'result_distribution': result_dist
        }


# ==================== æ•°æ®é‡‡é›†æ¨¡å— (Cloud Optimized) ====================

class DataCollector:
    """é€‚é…äº‘ç«¯çš„ä¿®å¤ç‰ˆæ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        self.driver = None
        self.base_urls = {
            'live': "https://live.500.com/",
            'europe': "https://odds.500.com/fenxi/ouzhi-{}.shtml",
            'handicap': "https://odds.500.com/fenxi/rangqiu-{}.shtml",
            'asia': "https://odds.500.com/fenxi/yazhi-{}.shtml",
            'daxiao': "https://odds.500.com/fenxi/daxiao-{}.shtml"
        }
        self.log_callback = None

    def set_log_callback(self, callback):
        self.log_callback = callback

    def _log(self, message):
        if self.log_callback:
            self.log_callback(message)
        print(message)

    def get_driver(self):
        """è·å–é…ç½®å¥½çš„ Chrome WebDriver - æ”¯æŒæœ¬åœ°å’Œäº‘ç«¯ç¯å¢ƒ"""
        if self.driver is not None:
            try:
                self.driver.current_url
                return self.driver
            except:
                self.close()

        try:
            options = Options()
            
            # æ— å¤´æ¨¡å¼ï¼ˆå¿…é¡»ï¼‰
            options.add_argument("--headless=new")
            options.add_argument("--no-sandbox")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--disable-features=NetworkService")
            options.add_argument("--window-size=1920,1080")
            options.add_argument("--disable-features=VizDisplayCompositor")
            
            # åæ£€æµ‹è®¾ç½®
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option('useAutomationExtension', False)
            
            # è®¾ç½® User-Agent
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.0")
            
            # ç¦ç”¨å›¾ç‰‡åŠ è½½ï¼ˆåŠ é€Ÿï¼‰
            options.add_experimental_option("prefs", {"profile.managed_default_content_settings.images": 2})
            
            # æ£€æµ‹ç¯å¢ƒå¹¶é…ç½®
            chromium_paths = [
                '/usr/bin/chromium',
                '/usr/bin/chromium-browser',
                '/usr/lib/chromium/chromium'
            ]
            
            chromedriver_paths = [
                '/usr/bin/chromedriver',
                '/usr/lib/chromium/chromedriver',
                '/usr/local/bin/chromedriver'
            ]
            
            chromium_binary = None
            for path in chromium_paths:
                if os.path.exists(path):
                    chromium_binary = path
                    break
            
            chromedriver_binary = None
            for path in chromedriver_paths:
                if os.path.exists(path):
                    chromedriver_binary = path
                    break
            
            if chromium_binary and chromedriver_binary:
                # äº‘ç«¯ç¯å¢ƒ
                self._log(f"â˜ï¸ ä½¿ç”¨äº‘ç«¯ Chromium: {chromium_binary}")
                options.binary_location = chromium_binary
                service = Service(chromedriver_binary)
                self.driver = webdriver.Chrome(service=service, options=options)
            else:
                # æœ¬åœ°ç¯å¢ƒ
                self._log("ğŸ’» ä½¿ç”¨æœ¬åœ° Chrome")
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=options)
            
            # éšè— webdriver å±æ€§
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': 'Object.defineProperty(navigator, "webdriver", {get: () => undefined})'
            })
            
            self.driver.set_page_load_timeout(30)
            return self.driver
            
        except Exception as e:
            self._log(f"âŒ æµè§ˆå™¨åˆ›å»ºå¤±è´¥: {str(e)[:100]}")
            import traceback
            self._log(traceback.format_exc()[:200])
            return None

    def close(self):
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None

    def get_page_with_retry(self, url, wait=3, max_retries=3):
        """å¸¦é‡è¯•çš„é¡µé¢è·å–"""
        for i in range(max_retries):
            try:
                driver = self.get_driver()
                if not driver:
                    time.sleep(2)
                    continue
                    
                driver.get(url)
                time.sleep(wait)
                
                # æ»šåŠ¨é¡µé¢ç¡®ä¿åŠ è½½å®Œæˆ
                for _ in range(2):
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(0.5)
                
                return driver.page_source
                
            except Exception as e:
                self._log(f"âš ï¸ ç¬¬ {i+1} æ¬¡å°è¯•å¤±è´¥: {str(e)[:50]}")
                self.close()
                time.sleep(3)
                
                if i == max_retries - 1:
                    self._log(f"âŒ æœ€ç»ˆå¤±è´¥: {url}")
                    return None
        
        return None

    def get_page(self, url, wait=3):
        """è·å–é¡µé¢"""
        return self.get_page_with_retry(url, wait, max_retries=3)

    def fetch_matches_by_date(self, date_str: str, only_finished: bool = True) -> pd.DataFrame:
        """è·å–æŒ‡å®šæ—¥æœŸçš„æ¯”èµ›æ•°æ®"""
        html = self.get_page(f"{self.base_urls['live']}?e={date_str}", wait=4)
        if not html:
            return pd.DataFrame()

        soup = BeautifulSoup(html, 'lxml')
        matches = []

        for idx, tr in enumerate(soup.find_all('tr', id=re.compile(r'^a\d+$'))):
            try:
                match_id_full = tr.get('id', '')
                match_id = match_id_full.replace('a', '') if match_id_full else ''
                if not match_id or not match_id.isdigit():
                    continue

                tds = tr.find_all('td')
                if len(tds) < 8:
                    continue

                status_code = tr.get('status', '')
                row_text = tr.get_text()

                status = "æœª"
                if status_code == '4' or 'å®Œ' in row_text:
                    status = "å®Œ"
                elif status_code == '2' or 'è¿›è¡Œä¸­' in row_text:
                    status = "è¿›è¡Œä¸­"

                if only_finished and status != "å®Œ":
                    continue

                league = tds[1].get_text(strip=True) if len(tds) > 1 else ""

                match_time = ""
                if len(tds) > 3:
                    time_text = tds[3].get_text(strip=True)
                    time_match = re.search(r'(\d{2}:\d{2})', time_text)
                    if time_match:
                        match_time = time_match.group(1)

                score_home, score_away, actual_result = "", "", ""
                if status == "å®Œ":
                    pk_div = tr.find("div", class_="pk")
                    if pk_div:
                        all_links = pk_div.find_all("a")
                        if len(all_links) >= 3:
                            try:
                                home_text = all_links[0].get_text(strip=True)
                                away_text = all_links[2].get_text(strip=True)
                                home_val = int(home_text)
                                away_val = int(away_text)
                                if 0 <= home_val <= 20 and 0 <= away_val <= 20:
                                    score_home = str(home_val)
                                    score_away = str(away_val)
                            except:
                                pass

                    if score_home and score_away:
                        try:
                            sh, sa = int(score_home), int(score_away)
                            if sh > sa:
                                actual_result = "ä¸»èƒœ"
                            elif sh < sa:
                                actual_result = "å®¢èƒœ"
                            else:
                                actual_result = "å¹³å±€"
                        except:
                            pass

                teams = []
                for t_idx in [5, 7]:
                    if t_idx < len(tds):
                        links = tds[t_idx].find_all('a')
                        for link in links:
                            name = link.get_text(strip=True)
                            if name and len(name) > 1 and not re.match(r'^\d+(\.\d+)?$', name):
                                if name not in teams:
                                    teams.append(name)
                                    break

                if len(teams) < 2:
                    for td in tds[2:]:
                        text = td.get_text(strip=True)
                        if (2 <= len(text) <= 15 and 
                            not any(c.isdigit() for c in text) and
                            text not in ['ä¸»', 'å®¢', 'vs', '-', ':', 'åŠçƒ', 'å¹³æ‰‹', 'å—åŠçƒ', 'å—å¹³æ‰‹', 'ä¸€çƒ']):
                            if text not in teams:
                                teams.append(text)
                        if len(teams) >= 2:
                            break

                teams = teams[:2]

                if len(teams) >= 2:
                    matches.append({
                        'order': idx,
                        'match_id': match_id,
                        'date': date_str,
                        'league': league or 'æœªçŸ¥',
                        'time': match_time,
                        'status': status,
                        'home_team': teams[0],
                        'away_team': teams[1],
                        'score': f"{score_home}-{score_away}" if score_home and score_away else "",
                        'score_home': score_home,
                        'score_away': score_away,
                        'actual_result': actual_result,
                        'has_result': status == "å®Œ" and actual_result != ""
                    })

            except Exception as e:
                continue

        df = pd.DataFrame(matches)
        if not df.empty:
            df = df.sort_values('order').reset_index(drop=True)
        return df

    def fetch_all_odds(self, match_id: str, log_callback=None) -> Dict:
        """è·å–æ‰€æœ‰å››ç§èµ”ç‡"""
        if log_callback:
            self.set_log_callback(log_callback)

        odds_data = {
            'europe': [],
            'asia': [],
            'handicap': [],
            'daxiao': []
        }

        self._log(f"ğŸ”„ å¼€å§‹è·å–æ¯”èµ› {match_id} çš„4ç§èµ”ç‡...")

        # 1. æ¬§æ´²èµ”ç‡
        self._log(f"ğŸ“Š [1/4] å¼€å§‹è·å–æ¬§æ´²èµ”ç‡...")
        try:
            odds_data['europe'] = self._fetch_europe_odds(match_id)
            self._log(f"âœ“ [1/4] æ¬§æ´²èµ”ç‡: {len(odds_data['europe'])} å®¶å…¬å¸")
        except Exception as e:
            self._log(f"âŒ [1/4] æ¬§æ´²èµ”ç‡è·å–å¤±è´¥: {str(e)[:50]}")

        time.sleep(0.5)

        # 2. äºšç›˜
        self._log(f"ğŸ“Š [2/4] å¼€å§‹è·å–äºšç›˜æ•°æ®...")
        try:
            odds_data['asia'] = self._fetch_asia_odds(match_id)
            self._log(f"âœ“ [2/4] äºšç›˜: {len(odds_data['asia'])} å®¶å…¬å¸")
        except Exception as e:
            self._log(f"âŒ [2/4] äºšç›˜è·å–å¤±è´¥: {str(e)[:50]}")

        time.sleep(0.5)

        # 3. è®©çƒèƒœå¹³è´Ÿ
        self._log(f"ğŸ“Š [3/4] å¼€å§‹è·å–è®©çƒèƒœå¹³è´Ÿ...")
        try:
            odds_data['handicap'] = self._fetch_handicap_odds(match_id)
            self._log(f"âœ“ [3/4] è®©çƒ: {len(odds_data['handicap'])} å®¶å…¬å¸")
        except Exception as e:
            self._log(f"âŒ [3/4] è®©çƒè·å–å¤±è´¥: {str(e)[:50]}")

        time.sleep(0.5)

        # 4. å¤§å°çƒ
        self._log(f"ğŸ“Š [4/4] å¼€å§‹è·å–å¤§å°çƒæ•°æ®...")
        try:
            odds_data['daxiao'] = self._fetch_daxiao_odds(match_id)
            self._log(f"âœ“ [4/4] å¤§å°çƒ: {len(odds_data['daxiao'])} å®¶å…¬å¸")
        except Exception as e:
            self._log(f"âŒ [4/4] å¤§å°çƒè·å–å¤±è´¥: {str(e)[:50]}")

        total = sum(len(v) for v in odds_data.values())
        self._log(f"âœ… æ€»è®¡: {total} æ¡èµ”ç‡æ•°æ®")

        return odds_data

    def _fetch_europe_odds(self, match_id: str) -> List[Dict]:
        """è·å–æ¬§æ´²èµ”ç‡"""
        url = f"https://odds.500.com/fenxi/ouzhi-{match_id}.shtml"
        self._log(f"      ğŸŒ è®¿é—®: {url}")

        html = self.get_page(url, wait=4)
        if not html:
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        try:
            rows = soup.find_all('tr', {'class': ['tr1', 'tr2']})
            
            for tr in rows:
                try:
                    company_name = ""
                    name_td = tr.find('td', {'class': 'tb_plgs'})
                    if name_td:
                        span = name_td.find('span', {'class': 'quancheng'})
                        if span:
                            company_name = span.get_text(strip=True)

                    if not company_name or any(x in company_name for x in ['å¹³å‡', 'æœ€å¤§', 'æœ€å°']):
                        continue

                    tds = tr.find_all('td', recursive=False)
                    if len(tds) < 3:
                        continue

                    odds_td = tds[2]
                    inner_table = odds_td.find('table', {'class': 'pl_table_data'})
                    if not inner_table:
                        continue

                    inner_rows = inner_table.find_all('tr')
                    if len(inner_rows) < 2:
                        continue

                    init_row = inner_rows[0]
                    init_tds = init_row.find_all('td')
                    if len(init_tds) < 3:
                        continue

                    init_home = self._parse_odds_value(init_tds[0])
                    init_draw = self._parse_odds_value(init_tds[1])
                    init_away = self._parse_odds_value(init_tds[2])

                    live_row = inner_rows[1]
                    live_tds = live_row.find_all('td')
                    if len(live_tds) < 3:
                        continue

                    live_home = self._parse_odds_value(live_tds[0])
                    live_draw = self._parse_odds_value(live_tds[1])
                    live_away = self._parse_odds_value(live_tds[2])

                    if all(v > 0 for v in [init_home, init_draw, init_away, live_home, live_draw, live_away]):
                        companies.append({
                            'company': company_name,
                            'init_home': round(init_home, 2),
                            'init_draw': round(init_draw, 2),
                            'init_away': round(init_away, 2),
                            'live_home': round(live_home, 2),
                            'live_draw': round(live_draw, 2),
                            'live_away': round(live_away, 2),
                            'change_home': round(live_home - init_home, 2),
                            'change_draw': round(live_draw - init_draw, 2),
                            'change_away': round(live_away - init_away, 2)
                        })

                except:
                    continue

        except Exception as e:
            self._log(f"      âŒ æå–å¤±è´¥: {str(e)[:50]}")

        return companies

    def _parse_odds_value(self, td) -> float:
        """ä»tdå…ƒç´ è§£æèµ”ç‡å€¼"""
        try:
            text = td.get_text(strip=True)
            text = re.sub(r'[â†‘â†“]', '', text).strip()
            return float(text)
        except:
            return 0.0

    def _fetch_asia_odds(self, match_id: str) -> List[Dict]:
        """è·å–äºšç›˜æ•°æ®"""
        url = f"https://odds.500.com/fenxi/yazhi-{match_id}.shtml"
        html = self.get_page(url, wait=3)
        if not html:
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        try:
            row_input = soup.find('input', {'name': 'row'})
            if row_input:
                row_value = row_input.get('value', '')
                if row_value:
                    for row in row_value.split('|'):
                        if not row or 'å…¬å¸' in row:
                            continue
                        parts = row.split(',')
                        if len(parts) >= 7:
                            try:
                                companies.append({
                                    'company': parts[0].strip(),
                                    'init_home': float(parts[1]),
                                    'init_handicap': parts[2].strip(),
                                    'init_away': float(parts[3]),
                                    'live_home': float(parts[4]),
                                    'live_handicap': parts[5].strip(),
                                    'live_away': float(parts[6])
                                })
                            except:
                                continue
        except:
            pass

        return companies

    def _fetch_handicap_odds(self, match_id: str) -> List[Dict]:
        """è·å–è®©çƒèƒœå¹³è´Ÿ"""
        url = f"https://odds.500.com/fenxi/rangqiu-{match_id}.shtml"
        html = self.get_page(url, wait=3)
        if not html:
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        try:
            row_input = soup.find('input', {'name': 'row'})
            if row_input:
                row_value = row_input.get('value', '')
                if row_value:
                    for row in row_value.split('|'):
                        if not row or 'å…¬å¸' in row:
                            continue
                        parts = row.split(',')
                        if len(parts) >= 8:
                            try:
                                companies.append({
                                    'company': parts[0].strip(),
                                    'handicap': parts[1].strip(),
                                    'init_home': float(parts[2]),
                                    'init_draw': float(parts[3]),
                                    'init_away': float(parts[4]),
                                    'live_home': float(parts[5]),
                                    'live_draw': float(parts[6]),
                                    'live_away': float(parts[7])
                                })
                            except:
                                continue
        except:
            pass

        return companies

    def _fetch_daxiao_odds(self, match_id: str) -> List[Dict]:
        """è·å–å¤§å°çƒæ•°æ®"""
        url = f"https://odds.500.com/fenxi/daxiao-{match_id}.shtml"
        html = self.get_page(url, wait=3)
        if not html:
            return []

        soup = BeautifulSoup(html, 'lxml')
        companies = []

        try:
            row_input = soup.find('input', {'name': 'row'})
            if row_input:
                row_value = row_input.get('value', '')
                if row_value:
                    for row in row_value.split('|'):
                        if not row or 'å…¬å¸' in row:
                            continue
                        parts = row.split(',')
                        if len(parts) >= 7:
                            try:
                                companies.append({
                                    'company': parts[0].strip(),
                                    'init_over': float(parts[1]),
                                    'init_line': parts[2].strip(),
                                    'init_under': float(parts[3]),
                                    'live_over': float(parts[4]),
                                    'live_line': parts[5].strip(),
                                    'live_under': float(parts[6])
                                })
                            except:
                                continue
        except:
            pass

        return companies

    def batch_fetch_history(self, start_date: str, days: int = 7, progress_callback=None, log_callback=None) -> pd.DataFrame:
        """æ‰¹é‡è·å–å†å²æ•°æ®"""
        if log_callback:
            self.set_log_callback(log_callback)

        all_matches = []
        start = datetime.strptime(start_date, '%Y-%m-%d')

        for i in range(days):
            current_date = start - timedelta(days=i)
            date_str = current_date.strftime('%Y-%m-%d')

            self._log(f"ğŸ“… è·å– {date_str} çš„æ¯”èµ›...")

            if progress_callback:
                progress_callback(i+1, days, date_str, "è·å–æ¯”èµ›åˆ—è¡¨...")

            matches = self.fetch_matches_by_date(date_str, only_finished=True)

            if not matches.empty:
                self._log(f"ğŸ“ æ‰¾åˆ° {len(matches)} åœºå®Œèµ›æ¯”èµ›")

                if progress_callback:
                    progress_callback(i+1, days, date_str, f"è·å– {len(matches)} åœºæ¯”èµ›èµ”ç‡...")

                match_list = []
                for idx, row in matches.iterrows():
                    if progress_callback:
                        progress_callback(i+1, days, date_str, f"[{idx+1}/{len(matches)}] {row['home_team']} vs {row['away_team']}")

                    odds = self.fetch_all_odds(row['match_id'], log_callback)

                    match_data = row.to_dict()
                    match_data.update(odds)
                    match_list.append(match_data)

                    time.sleep(0.2)

                all_matches.extend(match_list)
                self._log(f"âœ… {date_str} å®Œæˆï¼Œå…± {len(match_list)} åœº")
            else:
                self._log(f"âš ï¸ {date_str} æ— å®Œèµ›æ•°æ®")

            time.sleep(0.5)

        df = pd.DataFrame(all_matches)
        if not df.empty:
            df = df.sort_values(['date', 'order']).reset_index(drop=True)
        return df

    def fetch_future_matches(self, date_str: str) -> pd.DataFrame:
        """è·å–æŒ‡å®šæ—¥æœŸçš„æœªæ¥æ¯”èµ›"""
        html = self.get_page(f"{self.base_urls['live']}?e={date_str}", wait=4)
        if not html:
            return pd.DataFrame()

        soup = BeautifulSoup(html, 'lxml')
        matches = []

        for idx, tr in enumerate(soup.find_all('tr', id=re.compile(r'^a\d+$'))):
            try:
                match_id_full = tr.get('id', '')
                match_id = match_id_full.replace('a', '') if match_id_full else ''
                if not match_id or not match_id.isdigit():
                    continue

                tds = tr.find_all('td')
                if len(tds) < 8:
                    continue

                status_code = tr.get('status', '')
                row_text = tr.get_text()

                status = "æœª"
                if status_code == '4' or 'å®Œ' in row_text:
                    status = "å®Œ"
                elif status_code == '2' or 'è¿›è¡Œä¸­' in row_text:
                    status = "è¿›è¡Œä¸­"

                if status == "å®Œ":
                    continue

                league = tds[1].get_text(strip=True) if len(tds) > 1 else ""

                match_time = ""
                if len(tds) > 3:
                    time_text = tds[3].get_text(strip=True)
                    time_match = re.search(r'(\d{2}:\d{2})', time_text)
                    if time_match:
                        match_time = time_match.group(1)

                teams = []
                for t_idx in [5, 7]:
                    if t_idx < len(tds):
                        links = tds[t_idx].find_all('a')
                        for link in links:
                            name = link.get_text(strip=True)
                            if name and len(name) > 1 and not re.match(r'^\d+(\.\d+)?$', name):
                                if name not in teams:
                                    teams.append(name)
                                    break

                if len(teams) < 2:
                    for td in tds[2:]:
                        text = td.get_text(strip=True)
                        if (2 <= len(text) <= 15 and 
                            not any(c.isdigit() for c in text) and
                            text not in ['ä¸»', 'å®¢', 'vs', '-', ':']):
                            if text not in teams:
                                teams.append(text)
                        if len(teams) >= 2:
                            break

                teams = teams[:2]

                if len(teams) >= 2:
                    matches.append({
                        'order': idx,
                        'match_id': match_id,
                        'date': date_str,
                        'league': league or 'æœªçŸ¥',
                        'time': match_time,
                        'status': status,
                        'home_team': teams[0],
                        'away_team': teams[1],
                        'score': "",
                        'score_home': "",
                        'score_away': "",
                        'actual_result': "",
                        'has_result': False
                    })

            except Exception as e:
                continue

        df = pd.DataFrame(matches)
        if not df.empty:
            df = df.sort_values('order').reset_index(drop=True)
        return df

    def fetch_future_matches_with_odds(self, date_str: str, progress_callback=None, log_callback=None) -> pd.DataFrame:
        """è·å–æœªæ¥æ¯”èµ›å¹¶è·å–èµ”ç‡æ•°æ®"""
        if log_callback:
            self.set_log_callback(log_callback)

        self._log(f"ğŸ”® è·å– {date_str} çš„æœªæ¥æ¯”èµ›...")

        matches = self.fetch_future_matches(date_str)
        if matches.empty:
            self._log(f"âš ï¸ {date_str} æ²¡æœ‰æœªæ¥æ¯”èµ›")
            return pd.DataFrame()

        self._log(f"ğŸ“ æ‰¾åˆ° {len(matches)} åœºæœªæ¥æ¯”èµ›")

        match_list = []
        for idx, row in matches.iterrows():
            if progress_callback:
                progress_callback(idx+1, len(matches), date_str, f"{row['home_team']} vs {row['away_team']}")

            self._log(f"ğŸ“Š è·å–èµ”ç‡: {row['home_team']} vs {row['away_team']}")

            odds = self.fetch_all_odds(row['match_id'], log_callback)

            match_data = row.to_dict()
            match_data.update(odds)
            match_list.append(match_data)

            time.sleep(0.2)

        df = pd.DataFrame(match_list)
        if not df.empty:
            df = df.sort_values('order').reset_index(drop=True)

        self._log(f"âœ… {date_str} å®Œæˆï¼Œå…± {len(df)} åœº")
        return df


# ==================== ç‰¹å¾å·¥ç¨‹ ====================

class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹"""

    FEATURE_DIM = 85

    def __init__(self):
        self.scaler = StandardScaler()
        self.is_fitted = False

    def extract_features(self, match_data: Dict) -> np.ndarray:
        """æå–ç‰¹å¾"""
        features = []

        europe = match_data.get('europe', [])
        features.extend(self._europe_features(europe))

        asia = match_data.get('asia', [])
        features.extend(self._asia_features(asia))

        handicap = match_data.get('handicap', [])
        features.extend(self._handicap_features(handicap))

        daxiao = match_data.get('daxiao', [])
        features.extend(self._daxiao_features(daxiao))

        features.extend(self._meta_features(match_data))

        if len(features) > self.FEATURE_DIM:
            features = features[:self.FEATURE_DIM]
        elif len(features) < self.FEATURE_DIM:
            features.extend([0.0] * (self.FEATURE_DIM - len(features)))

        return np.array(features)

    def _europe_features(self, europe_odds):
        """æ¬§æ´²èµ”ç‡ç‰¹å¾"""
        if not europe_odds or len(europe_odds) == 0:
            return [0.0] * 35

        try:
            df = pd.DataFrame(europe_odds)
            features = []

            for col in ['live_home', 'live_draw', 'live_away']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()), 
                        float(df[col].std()) if len(df) > 1 else 0.0, 
                        float(df[col].min()), 
                        float(df[col].max()), 
                        float(df[col].median())
                    ])
                else:
                    features.extend([0.0] * 5)

            for col in ['change_home', 'change_draw', 'change_away']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()),
                        (df[col] < 0).sum() / len(df) if len(df) > 0 else 0.0
                    ])
                else:
                    features.extend([0.0, 0.0])

            if 'live_home' in df.columns and 'live_draw' in df.columns and 'live_away' in df.columns:
                total_prob = 1/df['live_home'] + 1/df['live_draw'] + 1/df['live_away']
                features.extend([
                    float(total_prob.mean()),
                    float(total_prob.std()) if len(total_prob) > 1 else 0.0,
                    float((1/df['live_home']).mean()),
                    float((1/df['live_draw']).mean()),
                    float((1/df['live_away']).mean()),
                    float((1/df['live_home']).std()) if len(df) > 1 else 0.0,
                    float((1/df['live_draw']).std()) if len(df) > 1 else 0.0,
                    float((1/df['live_away']).std()) if len(df) > 1 else 0.0,
                    (total_prob > 1.1).sum() / len(df),
                    (total_prob < 0.95).sum() / len(df),
                    float(df['live_home'].mean() / df['live_away'].mean()) if df['live_away'].mean() != 0 else 1.0,
                    float((df['live_home'] < df['live_away']).sum() / len(df)),
                    float(df['live_draw'].mean()),
                    float(len(df))
                ])
            else:
                features.extend([0.0] * 14)

            return features[:35]
        except:
            return [0.0] * 35

    def _asia_features(self, asia_odds):
        """äºšç›˜ç‰¹å¾"""
        if not asia_odds or len(asia_odds) == 0:
            return [0.0] * 15

        try:
            df = pd.DataFrame(asia_odds)
            features = []

            for col in ['live_home', 'live_away']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()), 
                        float(df[col].std()) if len(df) > 1 else 0.0, 
                        float(df[col].min()), 
                        float(df[col].max())
                    ])
                else:
                    features.extend([0.0] * 4)

            if 'live_handicap' in df.columns:
                handicaps = []
                for h in df['live_handicap']:
                    try:
                        h_str = str(h).replace('çƒ', '').replace('åŠ', '.5').replace('å¹³', '0')
                        handicaps.append(float(h_str))
                    except:
                        handicaps.append(0)

                if len(handicaps) > 0:
                    features.extend([
                        float(np.mean(handicaps)),
                        float(np.std(handicaps)) if len(handicaps) > 1 else 0.0,
                        float(max(handicaps)),
                        float(min(handicaps)),
                        (np.array(handicaps) > 0).sum() / len(handicaps),
                        (np.array(handicaps) < 0).sum() / len(handicaps),
                        len(set(handicaps)) / len(handicaps) if len(handicaps) > 0 else 0
                    ])
                else:
                    features.extend([0.0] * 7)
            else:
                features.extend([0.0] * 7)

            return features[:15]
        except:
            return [0.0] * 15

    def _handicap_features(self, handicap_odds):
        """è®©çƒç‰¹å¾"""
        if not handicap_odds or len(handicap_odds) == 0:
            return [0.0] * 10

        try:
            df = pd.DataFrame(handicap_odds)
            features = []

            for col in ['live_home', 'live_draw', 'live_away']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()), 
                        float(df[col].std()) if len(df) > 1 else 0.0, 
                        float(df[col].min()), 
                        float(df[col].max())
                    ])
                else:
                    features.extend([0.0] * 4)

            if 'live_home' in df.columns and 'live_away' in df.columns and len(df) > 0:
                features.extend([
                    (df['live_home'] < df['live_away']).sum() / len(df),
                    (df['live_home'] > df['live_away']).sum() / len(df)
                ])
            else:
                features.extend([0.0, 0.0])

            return features[:10]
        except:
            return [0.0] * 10

    def _daxiao_features(self, daxiao_odds):
        """å¤§å°çƒç‰¹å¾"""
        if not daxiao_odds or len(daxiao_odds) == 0:
            return [0.0] * 12

        try:
            df = pd.DataFrame(daxiao_odds)
            features = []

            for col in ['live_over', 'live_under']:
                if col in df.columns and len(df) > 0:
                    features.extend([
                        float(df[col].mean()), 
                        float(df[col].std()) if len(df) > 1 else 0.0, 
                        float(df[col].min()), 
                        float(df[col].max())
                    ])
                else:
                    features.extend([0.0] * 4)

            if 'live_line' in df.columns:
                lines = []
                for line in df['live_line']:
                    try:
                        line_str = str(line).replace('çƒ', '').replace('åŠ', '.5')
                        lines.append(float(line_str))
                    except:
                        lines.append(2.5)

                if len(lines) > 0:
                    features.extend([
                        float(np.mean(lines)),
                        float(np.std(lines)) if len(lines) > 1 else 0.0,
                        float(max(lines)),
                        float(min(lines))
                    ])
                else:
                    features.extend([0.0] * 4)
            else:
                features.extend([0.0] * 4)

            return features[:12]
        except:
            return [0.0] * 12

    def _meta_features(self, match_data):
        """å…ƒç‰¹å¾"""
        features = []

        league = match_data.get('league', '')
        tier = 3
        if any(x in league for x in ['è‹±è¶…','è¥¿ç”²','æ„ç”²','å¾·ç”²','æ³•ç”²','æ¬§å† ']): tier = 1
        elif any(x in league for x in ['è·ç”²','è‘¡è¶…','ä¿„è¶…','æ¯”ç”²','æ¬§ç½—å·´']): tier = 2
        features.append(float(tier))

        match_time = match_data.get('time', '15:00')
        try:
            hour = int(match_time.split(':')[0])
            features.extend([
                float(np.sin(2 * np.pi * hour / 24)),
                float(np.cos(2 * np.pi * hour / 24)),
                1.0 if 19 <= hour <= 23 else 0.0,
                1.0 if hour < 12 else 0.0
            ])
        except:
            features.extend([0.0, 0.0, 0.0, 0.0])

        europe = match_data.get('europe', [])
        if europe and len(europe) > 0:
            try:
                df = pd.DataFrame(europe)
                if 'live_home' in df.columns and 'live_away' in df.columns:
                    avg_home = df['live_home'].mean()
                    avg_away = df['live_away'].mean()
                    features.extend([
                        float(1/avg_home) if avg_home > 0 else 0.5,
                        float(1/avg_away) if avg_away > 0 else 0.5,
                        float(avg_away / avg_home) if avg_home > 0 else 1.0,
                        float((avg_home < avg_away).sum() / len(df)) if len(df) > 0 else 0.5
                    ])
                else:
                    features.extend([0.5, 0.5, 1.0, 0.5])
            except:
                features.extend([0.5, 0.5, 1.0, 0.5])
        else:
            features.extend([0.5, 0.5, 1.0, 0.5])

        for odds_type in ['europe', 'asia', 'handicap', 'daxiao']:
            odds = match_data.get(odds_type, [])
            features.append(float(len(odds)) if odds else 0.0)

        return features[:13]

    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        X, y_labels, metadata = [], [], []

        for _, row in df.iterrows():
            if not row.get('actual_result') or row['actual_result'] not in ['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ']:
                continue

            has_odds = any(len(row.get(ot, []) or []) > 0 for ot in ['europe', 'asia', 'handicap', 'daxiao'])
            if not has_odds:
                continue

            features = self.extract_features(row.to_dict())

            if len(features) != self.FEATURE_DIM:
                if len(features) > self.FEATURE_DIM:
                    features = features[:self.FEATURE_DIM]
                else:
                    features = np.concatenate([features, np.zeros(self.FEATURE_DIM - len(features))])

            X.append(features)

            result = row['actual_result']
            if result == 'ä¸»èƒœ':
                y_labels.append(0)
            elif result == 'å¹³å±€':
                y_labels.append(1)
            else:
                y_labels.append(2)

            metadata.append({
                'match_id': row['match_id'],
                'teams': f"{row['home_team']} vs {row['away_team']}",
                'date': row['date'],
                'league': row['league']
            })

        if len(X) == 0:
            return np.array([]), np.array([]), []

        X = np.array(X)
        y_labels = np.array(y_labels)

        unique_classes = np.unique(y_labels)
        class_counts = np.bincount(y_labels, minlength=3)

        if len(unique_classes) < 2:
            return np.array([]), np.array([]), []

        min_count = min(class_counts[class_counts > 0])
        if min_count < 5:
            X_new, y_new, meta_new = [], [], []

            for cls in range(3):
                mask = y_labels == cls
                X_cls = X[mask]
                y_cls = y_labels[mask]
                meta_cls = [metadata[i] for i in range(len(metadata)) if mask[i]]

                if len(X_cls) > 0 and len(X_cls) < 5:
                    repeat_times = (5 // len(X_cls)) + 1
                    X_cls = np.repeat(X_cls, repeat_times, axis=0)[:10]
                    y_cls = np.repeat(y_cls, repeat_times)[:10]
                    meta_cls = (meta_cls * repeat_times)[:10]

                if len(X_cls) > 0:
                    X_new.extend(X_cls)
                    y_new.extend(y_cls)
                    meta_new.extend(meta_cls)

            X = np.array(X_new)
            y_labels = np.array(y_new)
            metadata = meta_new

        y_onehot = np.zeros((len(y_labels), 3))
        for i, label in enumerate(y_labels):
            y_onehot[i, label] = 1

        if not self.is_fitted:
            X = self.scaler.fit_transform(X)
            self.is_fitted = True
        else:
            X = self.scaler.transform(X)

        return X, y_onehot, metadata

    def transform(self, match_data: Dict) -> np.ndarray:
        features = self.extract_features(match_data)

        if len(features) != self.FEATURE_DIM:
            if len(features) > self.FEATURE_DIM:
                features = features[:self.FEATURE_DIM]
            else:
                features = np.concatenate([features, np.zeros(self.FEATURE_DIM - len(features))])

        if self.is_fitted:
            features = self.scaler.transform(features.reshape(1, -1))[0]
        return features

    def save(self, path):
        with open(path, 'wb') as f:
            pickle.dump({'scaler': self.scaler, 'is_fitted': self.is_fitted}, f)

    def load(self, path):
        with open(path, 'rb') as f:
            data = pickle.load(f)
            self.scaler = data['scaler']
            self.is_fitted = data['is_fitted']


# ==================== æ·±åº¦å­¦ä¹ æ¨¡å‹ ====================

class DeepLearningModel:
    """æ·±åº¦å­¦ä¹ æ¨¡å‹"""
    
    def __init__(self, input_dim=85):
        self.input_dim = input_dim
        self.dnn_model = None
        self.rf_model = None
        self.gbdt_model = None
        self.is_trained = False
        self.training_history = []
    
    def build_models(self):
        if TF_AVAILABLE:
            self.dnn_model = self._build_dnn()
        
        self.rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, random_state=42, n_jobs=-1)
        self.gbdt_model = GradientBoostingClassifier(n_estimators=150, learning_rate=0.05, max_depth=6, random_state=42)
    
    def _build_dnn(self):
        model = Sequential([
            Dense(256, activation='relu', input_shape=(self.input_dim,)),
            BatchNormalization(),
            Dropout(0.4),
            Dense(128, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),
            Dense(3, activation='softmax')
        ])
        
        model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
        return model
    
    def train(self, X, y, validation_split=0.2, epochs=100):
        if len(X) < 10:
            return False, "æ•°æ®é‡ä¸è¶³"
        
        results = {}
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=validation_split, random_state=42)
        
        if TF_AVAILABLE and self.dnn_model:
            callbacks = [
                EarlyStopping(patience=15, restore_best_weights=True, monitor='val_accuracy'),
                ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)
            ]
            history = self.dnn_model.fit(X_train, y_train, validation_data=(X_val, y_val), 
                                        epochs=epochs, batch_size=32, callbacks=callbacks, verbose=0)
            results['dnn'] = {
                'train_acc': max(history.history['accuracy']),
                'val_acc': max(history.history['val_accuracy'])
            }
            self.training_history.append(history.history)
        
        y_train_labels = np.argmax(y_train, axis=1)
        y_val_labels = np.argmax(y_val, axis=1)
        
        self.rf_model.fit(X_train, y_train_labels)
        results['rf'] = {
            'train_acc': self.rf_model.score(X_train, y_train_labels),
            'val_acc': self.rf_model.score(X_val, y_val_labels)
        }
        
        self.gbdt_model.fit(X_train, y_train_labels)
        results['gbdt'] = {
            'train_acc': self.gbdt_model.score(X_train, y_train_labels),
            'val_acc': self.gbdt_model.score(X_val, y_val_labels)
        }
        
        self.is_trained = True
        return True, results
    
    def predict(self, X):
        if not self.is_trained:
            return None
        
        dnn_pred = None
        if TF_AVAILABLE and self.dnn_model:
            dnn_pred = self.dnn_model.predict(X.reshape(1, -1), verbose=0)[0]
        
        rf_pred = self.rf_model.predict_proba(X.reshape(1, -1))[0]
        gbdt_pred = self.gbdt_model.predict_proba(X.reshape(1, -1))[0]
        
        if dnn_pred is not None:
            ensemble_pred = 0.4 * dnn_pred + 0.3 * rf_pred + 0.3 * gbdt_pred
        else:
            ensemble_pred = 0.5 * rf_pred + 0.5 * gbdt_pred
        
        ensemble_pred = ensemble_pred / ensemble_pred.sum()
        
        labels = ['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ']
        pred_idx = np.argmax(ensemble_pred)
        
        # æ’åºè·å– Top 3
        sorted_indices = np.argsort(ensemble_pred)[::-1]
        top3_results = [(labels[i], round(ensemble_pred[i] * 100, 2)) for i in sorted_indices]
        
        # é¢„æµ‹æ¯”åˆ†
        score_predictions = self._predict_scores(ensemble_pred[0], ensemble_pred[1], ensemble_pred[2])
        
        # é¢„æµ‹æ€»è¿›çƒ
        total_goals_predictions = self._predict_total_goals(ensemble_pred[0], ensemble_pred[1], ensemble_pred[2])
        
        return {
            'result': labels[pred_idx],
            'confidence': round(ensemble_pred[pred_idx] * 100, 2),
            'probabilities': {labels[i]: round(ensemble_pred[i] * 100, 2) for i in range(3)},
            'top3_results': top3_results,
            'score_predictions': score_predictions,
            'total_goals_predictions': total_goals_predictions
        }
    
    def _predict_scores(self, home_win_prob, draw_prob, away_win_prob):
        """é¢„æµ‹æœ€å¯èƒ½çš„3ä¸ªæ¯”åˆ†"""
        scores = []

        if home_win_prob > 0.3:
            scores.extend([
                ('1-0', home_win_prob * 0.25),
                ('2-0', home_win_prob * 0.20),
                ('2-1', home_win_prob * 0.18),
                ('3-0', home_win_prob * 0.12),
                ('3-1', home_win_prob * 0.10),
            ])

        if draw_prob > 0.2:
            scores.extend([
                ('1-1', draw_prob * 0.40),
                ('0-0', draw_prob * 0.30),
                ('2-2', draw_prob * 0.20),
            ])

        if away_win_prob > 0.3:
            scores.extend([
                ('0-1', away_win_prob * 0.25),
                ('0-2', away_win_prob * 0.20),
                ('1-2', away_win_prob * 0.18),
                ('0-3', away_win_prob * 0.12),
                ('1-3', away_win_prob * 0.10),
            ])

        score_dict = {}
        for score, weight in scores:
            if score in score_dict:
                score_dict[score] += weight
            else:
                score_dict[score] = weight

        sorted_scores = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)
        top3 = sorted_scores[:3]

        total_weight = sum(w for _, w in top3) if top3 else 1
        return [(score, round(w/total_weight*100, 2)) for score, w in top3]

    def _predict_total_goals(self, home_win_prob, draw_prob, away_win_prob):
        """é¢„æµ‹æœ€å¯èƒ½çš„3ä¸ªæ€»è¿›çƒæ•°"""
        goals = []

        goals.extend([
            (0, draw_prob * 0.15 + (home_win_prob + away_win_prob) * 0.05),
            (1, (home_win_prob + away_win_prob) * 0.25),
            (2, draw_prob * 0.30 + (home_win_prob + away_win_prob) * 0.20),
            (3, draw_prob * 0.35 + (home_win_prob + away_win_prob) * 0.30),
            (4, draw_prob * 0.15 + (home_win_prob + away_win_prob) * 0.25),
            (5, (home_win_prob + away_win_prob) * 0.10),
            (6, (home_win_prob + away_win_prob) * 0.05),
        ])

        sorted_goals = sorted(goals, key=lambda x: x[1], reverse=True)
        top3 = sorted_goals[:3]

        total_weight = sum(w for _, w in top3) if top3 else 1
        return [(str(goals), round(w/total_weight*100, 2)) for goals, w in top3]

    def save(self, name='model_v5'):
        path = os.path.join(CONFIG.MODEL_DIR, name)
        os.makedirs(path, exist_ok=True)
        
        if TF_AVAILABLE and self.dnn_model:
            self.dnn_model.save(os.path.join(path, 'dnn.h5'))
        
        with open(os.path.join(path, 'rf.pkl'), 'wb') as f:
            pickle.dump(self.rf_model, f)
        with open(os.path.join(path, 'gbdt.pkl'), 'wb') as f:
            pickle.dump(self.gbdt_model, f)
        
        with open(os.path.join(path, 'config.pkl'), 'wb') as f:
            pickle.dump({'is_trained': self.is_trained, 'training_history': self.training_history}, f)
        
        return True
    
    def load(self, name='model_v5'):
        path = os.path.join(CONFIG.MODEL_DIR, name)
        
        if not os.path.exists(path):
            return False
        
        try:
            if TF_AVAILABLE and os.path.exists(os.path.join(path, 'dnn.h5')):
                self.dnn_model = load_model(os.path.join(path, 'dnn.h5'))
            
            with open(os.path.join(path, 'rf.pkl'), 'rb') as f:
                self.rf_model = pickle.load(f)
            with open(os.path.join(path, 'gbdt.pkl'), 'rb') as f:
                self.gbdt_model = pickle.load(f)
            
            with open(os.path.join(path, 'config.pkl'), 'rb') as f:
                config = pickle.load(f)
                self.is_trained = config['is_trained']
                self.training_history = config['training_history']
            
            return True
        except:
            return False


# ==================== ä¸»æ§åˆ¶ç³»ç»Ÿ ====================

class FootballPredictionSystem:
    """è¶³çƒé¢„æµ‹ä¸»ç³»ç»Ÿ"""
    
    def __init__(self):
        self.collector = DataCollector()
        self.feature_engineer = FeatureEngineer()
        self.model = DeepLearningModel(input_dim=FeatureEngineer.FEATURE_DIM)
        self.data_file = CONFIG.DATA_FILE
        
        self.df = self._load_data()
        
        if self.model.load():
            scaler_path = os.path.join(CONFIG.MODEL_DIR, 'scaler.pkl')
            if os.path.exists(scaler_path):
                self.feature_engineer.load(scaler_path)
    
    def _load_data(self):
        if os.path.exists(self.data_file):
            try:
                with open(self.data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                df = pd.DataFrame(data)
                
                required_cols = ['match_id', 'date', 'league', 'time', 'home_team', 'away_team', 'actual_result']
                for col in required_cols:
                    if col not in df.columns:
                        df[col] = ''
                
                for col in ['europe', 'asia', 'daxiao', 'handicap']:
                    if col in df.columns:
                        df[col] = df[col].apply(lambda x: json.loads(x) if isinstance(x, str) else x if isinstance(x, list) else [])
                    else:
                        df[col] = [[] for _ in range(len(df))]
                
                return df
            except Exception as e:
                print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
        
        return pd.DataFrame(columns=[
            'match_id', 'date', 'league', 'time', 'status', 'home_team', 'away_team',
            'score', 'score_home', 'score_away', 'actual_result', 'has_result',
            'europe', 'asia', 'daxiao', 'handicap', 'order'
        ])
    
    def _save_data(self):
        try:
            if self.df.empty:
                with open(self.data_file, 'w', encoding='utf-8') as f:
                    json.dump([], f, ensure_ascii=False, indent=2)
                return True
            
            df_copy = self.df.copy()
            for col in ['europe', 'asia', 'daxiao', 'handicap']:
                if col in df_copy.columns:
                    def serialize_odds(x):
                        if isinstance(x, list):
                            return json.dumps(x, ensure_ascii=False)
                        elif isinstance(x, str):
                            try:
                                json.loads(x)
                                return x
                            except:
                                return json.dumps([], ensure_ascii=False)
                        else:
                            return json.dumps([], ensure_ascii=False)

                    df_copy[col] = df_copy[col].apply(serialize_odds)
            
            with open(self.data_file, 'w', encoding='utf-8') as f:
                json.dump(df_copy.to_dict('records'), f, ensure_ascii=False, indent=2)

            return True
        except Exception as e:
            print(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def batch_collect_training_data(self, start_date: str, days: int = 7, progress_callback=None, log_callback=None):
        """æ‰¹é‡æ”¶é›†è®­ç»ƒæ•°æ®"""
        new_df = self.collector.batch_fetch_history(start_date, days, progress_callback, log_callback)
        
        if new_df.empty:
            return False, "æœªè·å–åˆ°æ•°æ®"
        
        if not self.df.empty and 'match_id' in self.df.columns:
            existing_ids = set(self.df['match_id'].tolist())
            new_df = new_df[~new_df['match_id'].isin(existing_ids)]
        
        if not new_df.empty:
            for col in self.df.columns:
                if col not in new_df.columns:
                    if col in ['europe', 'asia', 'daxiao', 'handicap']:
                        new_df[col] = [[] for _ in range(len(new_df))]
                    else:
                        new_df[col] = ['' for _ in range(len(new_df))]
            
            for col in new_df.columns:
                if col not in self.df.columns:
                    if col in ['europe', 'asia', 'daxiao', 'handicap']:
                        self.df[col] = [[] for _ in range(len(self.df))]
                    else:
                        self.df[col] = ['' for _ in range(len(self.df))]
            
            self.df = pd.concat([self.df, new_df], ignore_index=True)
            self._save_data()
            
            has_result = len(new_df[new_df['actual_result'] != ''])
            has_odds = len(new_df[new_df['europe'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)])
            
            return True, f"æ–°å¢ {len(new_df)} åœºï¼ˆæœ‰ç»“æœ: {has_result}åœºï¼Œæœ‰èµ”ç‡: {has_odds}åœºï¼‰ï¼Œç´¯è®¡ {len(self.df)} åœº"
        
        return True, "æ‰€æœ‰æ•°æ®å·²æ˜¯æœ€æ–°"

    def collect_future_matches(self, date_str: str, progress_callback=None, log_callback=None):
        """æ”¶é›†æœªæ¥æ¯”èµ›ç”¨äºé¢„æµ‹"""
        new_df = self.collector.fetch_future_matches_with_odds(date_str, progress_callback, log_callback)

        if new_df.empty:
            return False, "æœªè·å–åˆ°æœªæ¥æ¯”èµ›"

        if not self.df.empty and 'match_id' in self.df.columns:
            existing_ids = set(self.df['match_id'].tolist())
            new_df = new_df[~new_df['match_id'].isin(existing_ids)]

        if not new_df.empty:
            for col in self.df.columns:
                if col not in new_df.columns:
                    if col in ['europe', 'asia', 'daxiao', 'handicap']:
                        new_df[col] = [[] for _ in range(len(new_df))]
                    else:
                        new_df[col] = ['' for _ in range(len(new_df))]

            for col in new_df.columns:
                if col not in self.df.columns:
                    if col in ['europe', 'asia', 'daxiao', 'handicap']:
                        self.df[col] = [[] for _ in range(len(self.df))]
                    else:
                        self.df[col] = ['' for _ in range(len(self.df))]

            self.df = pd.concat([self.df, new_df], ignore_index=True)
            self._save_data()

            has_odds = len(new_df[new_df['europe'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)])

            return True, f"æ–°å¢ {len(new_df)} åœºæœªæ¥æ¯”èµ›ï¼ˆæœ‰èµ”ç‡: {has_odds}åœºï¼‰"

        return True, "æ‰€æœ‰æœªæ¥æ¯”èµ›å·²æ˜¯æœ€æ–°"

    def train_model(self):
        """è®­ç»ƒæ¨¡å‹"""
        if self.df.empty or 'actual_result' not in self.df.columns:
            return False, "æ²¡æœ‰è®­ç»ƒæ•°æ®"
        
        train_df = self.df[self.df['actual_result'].isin(['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ'])].copy()
        
        if len(train_df) < CONFIG.MIN_TRAIN_SAMPLES:
            return False, f"éœ€è¦è‡³å°‘{CONFIG.MIN_TRAIN_SAMPLES}åœºæœ‰ç»“æœçš„æ¯”èµ›ï¼Œå½“å‰{len(train_df)}åœº"
        
        has_odds_count = 0
        for idx, row in train_df.iterrows():
            for odds_type in ['europe', 'asia', 'handicap', 'daxiao']:
                odds = row.get(odds_type, [])
                if odds and len(odds) > 0:
                    has_odds_count += 1
                    break
        
        if has_odds_count == 0:
            return False, f"æœ‰ç»“æœçš„æ¯”èµ›: {len(train_df)}åœºï¼Œä½†æœ‰èµ”ç‡æ•°æ®çš„: 0åœºã€‚è¯·ç¡®ä¿æˆåŠŸè·å–äº†èµ”ç‡ä¿¡æ¯ã€‚"
        
        X, y, metadata = self.feature_engineer.prepare_training_data(train_df)
        
        if len(X) == 0:
            return False, f"ç‰¹å¾æå–å¤±è´¥ã€‚æœ‰èµ”ç‡çš„æ¯”èµ›: {has_odds_count}åœºï¼Œä½†æ— æ³•æå–æœ‰æ•ˆç‰¹å¾ã€‚"
        
        self.model.build_models()
        success, results = self.model.train(X, y)
        
        if success:
            self.model.save()
            self.feature_engineer.save(os.path.join(CONFIG.MODEL_DIR, 'scaler.pkl'))
            return True, results
        
        return False, results
    
    def predict(self, match_data):
        """é¢„æµ‹"""
        if not self.model.is_trained:
            return None, "æ¨¡å‹æœªè®­ç»ƒ"
        
        features = self.feature_engineer.transform(match_data)
        result = self.model.predict(features)
        return result, None
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡"""
        if self.df.empty:
            return {'total': 0, 'trainable': 0, 'model_ready': self.model.is_trained}
        
        if 'actual_result' not in self.df.columns:
            trainable = 0
        else:
            trainable = len(self.df[self.df['actual_result'].isin(['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ'])])
        
        return {
            'total': len(self.df),
            'trainable': trainable,
            'model_ready': self.model.is_trained
        }


# ==================== Streamlit UI ====================

def main():
    st.set_page_config(page_title="âš½ è¶³çƒæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ v5.2", layout="wide")
    
    # æ£€æµ‹è¿è¡Œç¯å¢ƒ
    is_cloud = os.path.exists('/mount') or os.path.exists('/usr/bin/chromium')
    
    st.markdown("""
    <style>
    .main-header { font-size: 2.5rem; font-weight: bold; color: #1f77b4; text-align: center; }
    </style>
    """, unsafe_allow_html=True)
    
    st.markdown('<div class="main-header">âš½ è¶³çƒæ™ºèƒ½é¢„æµ‹ç³»ç»Ÿ v5.2<br><small>äº‘ç«¯ä¼˜åŒ–ç‰ˆ</small></div>', unsafe_allow_html=True)
    
    if 'system' not in st.session_state:
        with st.spinner("ç³»ç»Ÿåˆå§‹åŒ–ä¸­..."):
            st.session_state['system'] = FootballPredictionSystem()
    
    system = st.session_state['system']
    stats = system.get_stats()
    
    with st.sidebar:
        st.header("ğŸ›ï¸ æ§åˆ¶é¢æ¿")
        
        # æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯
        if is_cloud:
            st.info("â˜ï¸ äº‘ç«¯è¿è¡Œæ¨¡å¼")
        else:
            st.info("ğŸ’» æœ¬åœ°è¿è¡Œæ¨¡å¼")
        
        st.subheader("ğŸ“š æ‰¹é‡è·å–è®­ç»ƒæ•°æ®")
        col_date, col_days = st.columns([2, 1])
        with col_date:
            start_date = st.date_input("å¼€å§‹æ—¥æœŸ", datetime.now() - timedelta(days=7))
        with col_days:
            days = st.number_input("å¤©æ•°", min_value=1, max_value=30, value=7)
        
        if 'fetch_logs' not in st.session_state:
            st.session_state['fetch_logs'] = []
        
        log_area = st.empty()
        if st.session_state['fetch_logs']:
            with log_area.container():
                st.markdown("ğŸ“‹ **è·å–æ—¥å¿—**")
                log_html = "<div style='height: 120px; overflow-y: auto; border: 1px solid #e0e0e0; border-radius: 5px; padding: 8px; background-color: #f8f9fa; font-family: monospace; font-size: 10px; line-height: 1.6;'>"
                for log in st.session_state['fetch_logs'][-50:]:
                    color = '#0066cc' if 'ğŸ“¡' in log else '#28a745' if 'âœ…' in log else '#dc3545' if 'âŒ' in log else '#333'
                    log_html += f"<div style='color: {color}; margin-bottom: 2px;'>{log}</div>"
                log_html += "</div>"
                st.markdown(log_html, unsafe_allow_html=True)
        
        # è·å–æœªæ¥æ¯”èµ›
        st.subheader("ğŸ”® è·å–æœªæ¥æ¯”èµ›")
        future_date = st.date_input("é€‰æ‹©æ¯”èµ›æ—¥æœŸ", datetime.now() + timedelta(days=1), key="future_date")

        if st.button("è·å–æœªæ¥æ¯”èµ›", use_container_width=True, type="primary"):
            st.session_state['fetch_logs'] = []

            progress_bar = st.progress(0)
            status_text = st.empty()

            def update_progress(current, total, date_str, status):
                progress_bar.progress(current / total if total > 0 else 0)
                status_text.text(f"[{current}/{total}] {status}")

            def add_log(message):
                st.session_state['fetch_logs'].append(message)
                if len(st.session_state['fetch_logs']) > 100:
                    st.session_state['fetch_logs'] = st.session_state['fetch_logs'][-100:]

            with st.spinner("è·å–æœªæ¥æ¯”èµ›ä¸­..."):
                success, msg = system.collect_future_matches(
                    future_date.strftime('%Y-%m-%d'),
                    update_progress,
                    add_log
                )

            progress_bar.empty()
            status_text.empty()

            if success:
                st.success(msg)
                st.balloons()
            else:
                st.error(msg)

            st.rerun()

        st.markdown("---")

        if st.button("ğŸš€ æ‰¹é‡è·å–è®­ç»ƒæ•°æ®", use_container_width=True, type="primary"):
            st.session_state['fetch_logs'] = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            def update_progress(current_day, total_days, date_str, status):
                progress_bar.progress(current_day / total_days)
                status_text.text(f"[{current_day}/{total_days}] {date_str}: {status}")
            
            def add_log(message):
                st.session_state['fetch_logs'].append(message)
                if len(st.session_state['fetch_logs']) > 100:
                    st.session_state['fetch_logs'] = st.session_state['fetch_logs'][-100:]
            
            with st.spinner("è·å–ä¸­..."):
                success, msg = system.batch_collect_training_data(
                    start_date.strftime('%Y-%m-%d'),
                    int(days),
                    update_progress,
                    add_log
                )
            
            progress_bar.empty()
            status_text.empty()
            
            if success:
                st.success(msg)
                st.balloons()
            else:
                st.error(msg)
            
            st.rerun()
        
        st.markdown("---")
        
        st.subheader("ğŸ§  æ¨¡å‹è®­ç»ƒ")
        trainable_count = stats['trainable']
        
        if trainable_count >= CONFIG.MIN_TRAIN_SAMPLES:
            st.success(f"âœ… å¯è®­ç»ƒæ•°æ®: {trainable_count}åœº")
            if st.button("å¼€å§‹è®­ç»ƒæ¨¡å‹", use_container_width=True, type="primary"):
                with st.spinner("è®­ç»ƒä¸­..."):
                    success, result = system.train_model()
                    if success:
                        st.success("è®­ç»ƒå®Œæˆï¼")
                        st.json(result)
                    else:
                        st.error(result)
        else:
            st.warning(f"âš ï¸ éœ€è¦{CONFIG.MIN_TRAIN_SAMPLES}åœºï¼Œå½“å‰{trainable_count}åœº")
            st.progress(trainable_count / CONFIG.MIN_TRAIN_SAMPLES if CONFIG.MIN_TRAIN_SAMPLES > 0 else 0)
        
        st.markdown("---")
        
        if st.button("ğŸ’¾ ä¿å­˜æ•°æ®", use_container_width=True):
            if system._save_data():
                st.success("å·²ä¿å­˜")
        
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºæ•°æ®", use_container_width=True):
            system.df = pd.DataFrame(columns=[
                'match_id', 'date', 'league', 'time', 'status', 'home_team', 'away_team',
                'score', 'score_home', 'score_away', 'actual_result', 'has_result',
                'europe', 'asia', 'daxiao', 'handicap', 'order'
            ])
            system._save_data()
            st.success("å·²æ¸…ç©º")
    
    tabs = st.tabs(["ğŸ“Š æ•°æ®æ¦‚è§ˆ", "ğŸ¯ é¢„æµ‹ä¸­å¿ƒ"])
    
    with tabs[0]:
        st.subheader("è®­ç»ƒæ•°æ®æ¦‚è§ˆ")
        
        cols = st.columns(4)
        with cols[0]:
            st.metric("æ€»æ¯”èµ›æ•°", stats['total'])
        with cols[1]:
            st.metric("å¯è®­ç»ƒæ•°æ®", stats['trainable'])
        with cols[2]:
            st.metric("è®­ç»ƒé—¨æ§›", CONFIG.MIN_TRAIN_SAMPLES)
        with cols[3]:
            st.metric("æ¨¡å‹çŠ¶æ€", "âœ… å°±ç»ª" if stats['model_ready'] else "âŒ æœªè®­ç»ƒ")
        
        if not system.df.empty:
            st.markdown("---")
            
            display_df = system.df.copy()
            if 'actual_result' in display_df.columns:
                display_df = display_df[display_df['actual_result'].isin(['ä¸»èƒœ', 'å¹³å±€', 'å®¢èƒœ'])]
            
            display_cols = ['date', 'league', 'home_team', 'away_team', 'score', 'actual_result']
            display_cols = [c for c in display_cols if c in display_df.columns]

            def get_odds_status(row):
                odds_types = []
                for ot in ['europe', 'asia', 'handicap', 'daxiao']:
                    if ot in row and isinstance(row[ot], list) and len(row[ot]) > 0:
                        odds_types.append(ot[:2])
                return ','.join(odds_types) if odds_types else 'æ— '

            if not display_df.empty:
                display_df['odds'] = display_df.apply(get_odds_status, axis=1)
                display_cols.append('odds')
            
            if not display_df.empty:
                if 'order' in display_df.columns:
                    display_df = display_df.sort_values(['date', 'order'])
                
                st.dataframe(display_df[display_cols], use_container_width=True, height=400)
                
                st.markdown("---")
                st.subheader("èµ”ç‡æ•°æ®ç»Ÿè®¡")
                odds_stats = {}
                for odds_type in ['europe', 'asia', 'handicap', 'daxiao']:
                    if odds_type in display_df.columns:
                        count = display_df[odds_type].apply(lambda x: len(x) > 0 if isinstance(x, list) else False).sum()
                        odds_stats[odds_type] = count
                
                if odds_stats:
                    st.write(odds_stats)
            else:
                st.info("æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
        else:
            st.info("æš‚æ— æ•°æ®ï¼Œè¯·ä½¿ç”¨ä¾§è¾¹æ è·å–è®­ç»ƒæ•°æ®")
    
    with tabs[1]:
        st.subheader("æ¯”èµ›é¢„æµ‹")
        
        if not system.df.empty and stats['model_ready']:
            future_matches = system.df[system.df['actual_result'] == ''] if 'actual_result' in system.df.columns else system.df
            
            if not future_matches.empty:
                selected = st.selectbox(
                    "é€‰æ‹©æ¯”èµ›è¿›è¡Œé¢„æµ‹",
                    future_matches.to_dict('records'),
                    format_func=lambda x: f"{x.get('date', 'N/A')} | {x.get('league', 'N/A')} | {x.get('home_team', 'N/A')} vs {x.get('away_team', 'N/A')}"
                )
                
                if selected:
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.markdown(f"### {selected.get('home_team', 'N/A')} ğŸ†š {selected.get('away_team', 'N/A')}")
                        st.write(f"**è”èµ›:** {selected.get('league', 'N/A')}")
                        
                        europe_odds = selected.get('europe', [])
                        has_odds = europe_odds and len(europe_odds) > 0
                        if has_odds:
                            st.write(f"**èµ”ç‡æ•°æ®:** âœ… å·²è·å– ({len(europe_odds)}å®¶å…¬å¸)")
                        else:
                            st.warning("âš ï¸ æš‚æ— èµ”ç‡æ•°æ®")
                    
                    with col2:
                        if has_odds:
                            if st.button("ğŸ”® å¼€å§‹é¢„æµ‹", type="primary", use_container_width=True):
                                result, error = system.predict(selected)
                                if error:
                                    st.error(error)
                                else:
                                    st.success(f"**é¢„æµ‹ç»“æœ: {result['result']}**")
                                    st.write(f"**ç½®ä¿¡åº¦:** {result['confidence']}%")

                                    st.markdown("**èƒœå¹³è´Ÿæ¦‚ç‡ Top 3:**")
                                    for i, (res, conf) in enumerate(result['top3_results'], 1):
                                        st.write(f"{i}. {res}: {conf}%")

                                    st.markdown("**æœ€å¯èƒ½æ¯”åˆ† Top 3:**")
                                    for i, (score, prob) in enumerate(result['score_predictions'], 1):
                                        st.write(f"{i}. {score}: {prob}%")

                                    st.markdown("**æ€»è¿›çƒæ•° Top 3:**")
                                    for i, (goals, prob) in enumerate(result['total_goals_predictions'], 1):
                                        st.write(f"{i}. {goals}çƒ: {prob}%")
                        else:
                            st.info("è¯·å…ˆè·å–èµ”ç‡æ•°æ®")
            else:
                st.info("å½“å‰æ²¡æœ‰æœªé¢„æµ‹çš„æ¯”èµ›")
        else:
            if not stats['model_ready']:
                st.warning("âš ï¸ æ¨¡å‹æœªè®­ç»ƒ")
            else:
                st.info("æš‚æ— æ¯”èµ›æ•°æ®")

if __name__ == "__main__":
    main()